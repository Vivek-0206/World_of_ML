{
  
    
        "post0": {
            "title": "Dog_VS_Cat",
            "content": "Import tools . import tensorflow as tf import tensorflow_hub as hub . print(&quot;TF version:&quot;, tf.__version__) print(&quot;TF Hub version:&quot;, hub.__version__) . TF version: 2.3.0 TF Hub version: 0.10.0 . Getting our data ready . Read csv file . import pandas as pd labels_csv = pd.read_csv(&quot;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/Dog_vs_cat.csv&quot;) . labels_csv.drop(&#39;Unnamed: 0&#39;,axis=1, inplace=True) . labels_csv . filename category . 0 dog.9594.jpg | 1 | . 1 dog.9601.jpg | 1 | . 2 dog.9595.jpg | 1 | . 3 dog.9596.jpg | 1 | . 4 dog.96.jpg | 1 | . ... ... | ... | . 25006 cat.10143.jpg | 0 | . 25007 cat.10142.jpg | 0 | . 25008 cat.10153.jpg | 0 | . 25009 cat.10150.jpg | 0 | . 25010 cat.10148.jpg | 0 | . 25011 rows × 2 columns . Shuffled DataFrames. . labels_csv = labels_csv.sample(frac = 1) labels_csv . filename category . 18208 cat.4922.jpg | 0 | . 472 dog.9184.jpg | 1 | . 15617 cat.7580.jpg | 0 | . 10547 dog.12107.jpg | 1 | . 13962 cat.8578.jpg | 0 | . ... ... | ... | . 7820 dog.255.jpg | 1 | . 2677 dog.786.jpg | 1 | . 19737 cat.3836.jpg | 0 | . 22321 cat.12439.jpg | 0 | . 20540 cat.288.jpg | 0 | . 25011 rows × 2 columns . import matplotlib.pyplot as plt labels_csv[&quot;category&quot;].value_counts().plot.bar(figsize=(5, 5)); . from IPython.display import Image Image(&quot;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.10.jpg&quot;) . filenames = [&quot;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/&quot; + fname for fname in labels_csv[&quot;filename&quot;]] # Check the first 15 filenames[:15] . [&#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/cat.4922.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.9184.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/cat.7580.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.12107.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/cat.8578.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/cat.3540.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/cat.4413.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/cat.6806.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.6837.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.22.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.7478.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.10287.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/cat.8332.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.4007.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.5095.jpg&#39;] . Image(filenames[2509]) . labels_csv[&quot;category&quot;][2509] . 1 . let&#39;s prepare our labels. . import numpy as np labels = labels_csv[&quot;category&quot;].to_numpy() labels . array([0, 1, 0, ..., 0, 0, 0]) . len(labels) . 25011 . if len(labels) == len(filenames): print(&quot;labels matches number of filenames!&quot;) else: print(&quot;labels does not match number of filenames&quot;) . labels matches number of filenames! . unique_category = np.unique(labels) len(unique_category) . 2 . unique_category . array([0, 1]) . Turn label into an array of booleans . print(labels[0]) labels[0] == unique_category . 0 . array([ True, False]) . boolean_labels = [label == unique_category for label in labels] boolean_labels[:2] . [array([ True, False]), array([False, True])] . len(boolean_labels) . 25011 . print(labels[0]) print(np.where(unique_category == labels[0])) print(boolean_labels[0].argmax()) print(boolean_labels[0].astype(int)) . 0 (array([0]),) 0 [1 0] . filenames[:10] . [&#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/cat.4922.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.9184.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/cat.7580.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.12107.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/cat.8578.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/cat.3540.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/cat.4413.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/cat.6806.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.6837.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.22.jpg&#39;] . Creating our own validation set . X = filenames y = boolean_labels . from sklearn.model_selection import train_test_split # Into train and valid X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) len(X_train), len(X_val), len(y_train), len(y_val) . (20008, 5003, 20008, 5003) . X_train[:5],y_train[:5] . ([&#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.3851.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/cat.9936.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.10212.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/dog.85.jpg&#39;, &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/train/cat.4980.jpg&#39;], [array([False, True]), array([ True, False]), array([False, True]), array([False, True]), array([ True, False])]) . Preprocessing Images . Befor we do, let&#39;s see what image looks like in to tensors . from matplotlib.pyplot import imread image = imread(filenames[42]) image.shape . (194, 320, 3) . tf.constant(image) . &lt;tf.Tensor: shape=(194, 320, 3), dtype=uint8, numpy= array([[[225, 210, 203], [226, 211, 204], [228, 213, 206], ..., [202, 189, 183], [214, 201, 195], [219, 204, 199]], [[222, 207, 200], [223, 208, 201], [224, 209, 202], ..., [199, 186, 180], [212, 199, 193], [209, 194, 189]], [[223, 208, 201], [224, 209, 202], [224, 209, 202], ..., [206, 193, 187], [220, 207, 201], [208, 193, 188]], ..., [[228, 213, 206], [228, 213, 206], [230, 215, 208], ..., [231, 222, 217], [225, 216, 211], [218, 209, 204]], [[224, 209, 202], [228, 213, 206], [232, 217, 210], ..., [224, 215, 208], [225, 216, 207], [227, 218, 209]], [[224, 209, 202], [228, 213, 206], [232, 217, 210], ..., [224, 215, 208], [225, 216, 207], [227, 218, 209]]], dtype=uint8)&gt; . TURNING IMAGES INTO TENSOR . A Function for Preprocessing images: . Take an image filepath as input | Use Tensorflow to read the file and save it to a variable image | Turn our image into Tensors | Normalize our image | Resize the images to be a shape of (224,224) | Return the modified images | IMG_SIZE = 224 # Function def process_image(image_path, image_size=IMG_SIZE): &quot;&quot;&quot; Takes an image file path and turns the image into a Tensor. &quot;&quot;&quot; # Read in an image file image = tf.io.read_file(image_path) # Turn the jpg image into numerical Tensor with 3 colour channel(RGB) image = tf.image.decode_jpeg(image,channels=3) # Convert the color channel values to (0-1) values image = tf.image.convert_image_dtype(image,tf.float32) # Resize the image to (224,224) image = tf.image.resize(image, size=[image_size,image_size]) return image . Turning our data into Batches . def get_image_lable(image_path,label): &quot;&quot;&quot; Takes an image file path name and the label, processes the image and return a tuple (image, label). &quot;&quot;&quot; image = process_image(image_path) return image,label . Let&#39;s make a function to turn all of data into batches! . BATCH_SIZE = 32 # Function to convert data into batches def create_data_batches(X,y=None, batch_size=BATCH_SIZE,valid_data=False): &quot;&quot;&quot; Creates batches of data of image (X) and label (y) pairs. Shuffle the data if it&#39;s training data but doesn&#39;t shuffle if it&#39;s validation data. &quot;&quot;&quot; # If data is valid dataset (NO SHUFFLE) if valid_data: print(&quot;Creating valid data batches.........&quot;) data = tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(y))) data_batch = data.map(get_image_lable).batch(batch_size) return data_batch else: print(&quot;Creating train data batches.........&quot;) # Turn filepaths and labels into Tensors data = tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(y))) # Shuffling pathname and labels before mapping image processor fun data = data.shuffle(buffer_size=len(X)) data_batch = data.map(get_image_lable).batch(batch_size) return data_batch . train_data = create_data_batches(X_train, y_train) val_data = create_data_batches(X_val, y_val, valid_data=True) . Creating train data batches......... Creating valid data batches......... . train_data.element_spec, val_data.element_spec . ((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.bool, name=None)), (TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.bool, name=None))) . Visualizing Data Batches . import matplotlib.pyplot as plt # Create fun for viewing in a data batch def show_images(images, labels): &quot;&quot;&quot; Displays a plot of 25 images and their labels from a data batch. &quot;&quot;&quot; plt.figure(figsize=(20, 20)) for i in range(25): # Subplot ax = plt.subplot(5,5,i+1) plt.imshow(images[i]) plt.title(unique_category[labels[i].argmax()]) plt.axis(&quot;Off&quot;) . train_images, train_labels = next(train_data.as_numpy_iterator()) train_images, train_labels . (array([[[[0.2826692 , 0.27675182, 0.30912226], [0.28703934, 0.28112197, 0.31349242], [0.2952971 , 0.28937972, 0.32175016], ..., [0.17186178, 0.16404086, 0.20467226], [0.1409928 , 0.133163 , 0.17379884], [0.14484434, 0.14870942, 0.1844739 ]], [[0.31613198, 0.29538617, 0.333674 ], [0.30875242, 0.2880066 , 0.32629442], [0.3241037 , 0.3033579 , 0.34164572], ..., [0.14443067, 0.13221769, 0.15652625], [0.0981044 , 0.08584315, 0.11016917], [0.17427143, 0.16930833, 0.19169979]], [[0.32277972, 0.28142816, 0.32776922], [0.35427544, 0.3129239 , 0.35926494], [0.36919445, 0.32784292, 0.37418395], ..., [0.2003182 , 0.18230036, 0.19426078], [0.1211253 , 0.10258538, 0.11506787], [0.16337338, 0.14483346, 0.15731597]], ..., [[0.5920565 , 0.57494986, 0.6840207 ], [0.59542924, 0.5783353 , 0.6873584 ], [0.6164389 , 0.5993577 , 0.708333 ], ..., [0.7857931 , 0.80540097, 0.91912645], [0.7983909 , 0.81799877, 0.93172425], [0.7966274 , 0.81623524, 0.9299607 ]], [[0.5510833 , 0.5305476 , 0.646339 ], [0.5436558 , 0.524637 , 0.63493735], [0.5932025 , 0.57419705, 0.6844491 ], ..., [0.79417616, 0.8201229 , 0.9309161 ], [0.79588056, 0.8217882 , 0.9325988 ], [0.7838864 , 0.8150447 , 0.92253506]], [[0.5902488 , 0.5667194 , 0.6765233 ], [0.58125216, 0.55772275, 0.66066813], [0.59609354, 0.5725641 , 0.67550063], ..., [0.77784693, 0.81559896, 0.9219692 ], [0.7797044 , 0.8174431 , 0.92381775], [0.78092784, 0.82698435, 0.9226055 ]]], [[[0.44143307, 0.39829582, 0.39045268], [0.43832287, 0.39518562, 0.38734248], [0.43936062, 0.39622337, 0.38838023], ..., [0.6614712 , 0.735981 , 0.24427053], [0.61897016, 0.69347996, 0.20176955], [0.7173941 , 0.791904 , 0.3001935 ]], [[0.416229 , 0.37309173, 0.3652486 ], [0.41362923, 0.37049195, 0.36264881], [0.41362923, 0.37049195, 0.36264881], ..., [0.67969424, 0.7469824 , 0.3040377 ], [0.604985 , 0.67227316, 0.22932845], [0.6605307 , 0.72781885, 0.28487417]], [[0.4107143 , 0.36757702, 0.35973388], [0.40811452, 0.36497724, 0.3571341 ], [0.40811452, 0.36497724, 0.3571341 ], ..., [0.48873878, 0.52800286, 0.15470313], [0.46664834, 0.504557 , 0.13133964], [0.5523329 , 0.5916963 , 0.2183906 ]], ..., [[0.257373 , 0.27105847, 0.06162212], [0.39397824, 0.41317147, 0.20155296], [0.5798493 , 0.6104076 , 0.3826898 ], ..., [0.29091948, 0.35425556, 0.15920989], [0.25120127, 0.31705973, 0.10619664], [0.31459087, 0.3833176 , 0.16046302]], [[0.31542486, 0.33775508, 0.11932267], [0.42254394, 0.46245956, 0.21355283], [0.70242745, 0.7582074 , 0.44450924], ..., [0.2555019 , 0.3261044 , 0.11227396], [0.29704627, 0.36061937, 0.1480565 ], [0.20160262, 0.26611367, 0.05258301]], [[0.61772764, 0.6373793 , 0.3941107 ], [0.74314606, 0.7904675 , 0.46802348], [0.52938443, 0.6214713 , 0.1596628 ], ..., [0.23504041, 0.28994235, 0.08209924], [0.207591 , 0.24310353, 0.0585277 ], [0.20691648, 0.23181117, 0.06309658]]], [[[0.8067808 , 0.5491867 , 0.7598499 ], [0.5959661 , 0.271777 , 0.5144344 ], [0.7109116 , 0.28240204, 0.55941516], ..., [0.6609296 , 0.7001453 , 0.6648512 ], [0.6842579 , 0.71184 , 0.68042374], [0.6211969 , 0.65124774, 0.6120756 ]], [[0.7346151 , 0.12982179, 0.45797306], [0.83824706, 0.22100466, 0.5535414 ], [0.7474404 , 0.10629252, 0.44568238], ..., [0.45346475, 0.49268043, 0.4573863 ], [0.62152267, 0.66073835, 0.62544423], [0.7129607 , 0.7469767 , 0.7064828 ]], [[0.6097017 , 0.10229574, 0.3921245 ], [0.56863743, 0.08694429, 0.35841337], [0.58433217, 0.08333191, 0.37190062], ..., [0.392608 , 0.43966678, 0.40045112], [0.64723563, 0.68653864, 0.65120083], [0.63717455, 0.6711905 , 0.63069665]], ..., [[0.35166317, 0.3477416 , 0.33989847], [0.36074054, 0.35681897, 0.34897584], [0.34611344, 0.33950457, 0.3397234 ], ..., [0.21258524, 0.23058246, 0.12174098], [0.21291845, 0.23091568, 0.12445551], [0.20145264, 0.2194499 , 0.11043367]], [[0.3351363 , 0.33121473, 0.3233716 ], [0.34640884, 0.34248728, 0.33464414], [0.33810344, 0.33149454, 0.33171338], ..., [0.2615026 , 0.2857153 , 0.13704497], [0.23887654, 0.26243013, 0.11680023], [0.21524014, 0.23950122, 0.09060777]], [[0.3070396 , 0.30311802, 0.29527488], [0.32539397, 0.3214724 , 0.31362927], [0.32797605, 0.32136717, 0.321586 ], ..., [0.26418078, 0.2877102 , 0.14534342], [0.25512934, 0.27865875, 0.13744017], [0.23413137, 0.25766078, 0.11520979]]], ..., [[[0.36886755, 0.35710284, 0.337495 ], [0.37024313, 0.35847843, 0.33887056], [0.6482143 , 0.6364496 , 0.61684173], ..., [0.22596239, 0.24023049, 0.25483152], [0.23179743, 0.2474837 , 0.25924838], [0.2436975 , 0.25938377, 0.27114847]], [[0.376213 , 0.36444828, 0.34484044], [0.3416942 , 0.3299295 , 0.31032163], [0.6254352 , 0.61367047, 0.5940626 ], ..., [0.24108844, 0.25535655, 0.26995757], [0.24645104, 0.2621373 , 0.273902 ], [0.24873951, 0.26442578, 0.2761905 ]], [[0.3669993 , 0.3552346 , 0.33562675], [0.33851543, 0.32675073, 0.30714288], [0.6753051 , 0.6635404 , 0.6439326 ], ..., [0.25061226, 0.26488036, 0.27948138], [0.2546969 , 0.27038318, 0.28214788], [0.25378153, 0.2694678 , 0.2812325 ]], ..., [[0.69682366, 0.5975066 , 0.6113621 ], [0.6795593 , 0.58971846, 0.6009705 ], [0.66809714, 0.60013 , 0.6008453 ], ..., [0.3054788 , 0.21627113, 0.2467259 ], [0.34534314, 0.23756042, 0.2680653 ], [0.3389354 , 0.22016795, 0.24848439]], [[0.67655396, 0.56196076, 0.57270765], [0.6486129 , 0.54638463, 0.55407786], [0.6345677 , 0.5559213 , 0.5547535 ], ..., [0.32925826, 0.22821364, 0.24691615], [0.36057922, 0.2431775 , 0.25678056], [0.35966367, 0.23585422, 0.25143814]], [[0.7109322 , 0.5883931 , 0.5986823 ], [0.66681445, 0.5537442 , 0.5611296 ], [0.59691626, 0.50954384, 0.5077681 ], ..., [0.33136725, 0.22392224, 0.23819034], [0.34379455, 0.22348174, 0.23081224], [0.35798317, 0.22577025, 0.23081224]]], [[[0.28135863, 0.23037826, 0.26959392], [0.27375028, 0.2227699 , 0.258064 ], [0.2710955 , 0.2201151 , 0.25477895], ..., [0.5006653 , 0.45752805, 0.44968492], [0.48627454, 0.4431373 , 0.43529415], [0.48627454, 0.4431373 , 0.43529415]], [[0.30496526, 0.25398487, 0.28927898], [0.3158937 , 0.26491332, 0.30017772], [0.30290478, 0.2519244 , 0.27937537], ..., [0.50978786, 0.4666506 , 0.45880747], [0.4941702 , 0.44711137, 0.44711137], [0.4941702 , 0.44711137, 0.44711137]], [[0.2945208 , 0.2435404 , 0.2735474 ], [0.31782216, 0.26684177, 0.2946807 ], [0.29444796, 0.24611112, 0.26563144], ..., [0.5009104 , 0.45777312, 0.44992998], [0.5005953 , 0.45353645, 0.454339 ], [0.5005953 , 0.45353645, 0.45882356]], ..., [[0.36579135, 0.31873253, 0.32657567], [0.37657672, 0.3295179 , 0.33736104], [0.37433967, 0.32759598, 0.32696572], ..., [0.61843264, 0.5988248 , 0.5831385 ], [0.62185025, 0.60264367, 0.58007824], [0.61859286, 0.6 , 0.5750001 ]], [[0.37738097, 0.33032215, 0.33816528], [0.3815976 , 0.33453876, 0.3423819 ], [0.3566018 , 0.3098581 , 0.30922785], ..., [0.6313036 , 0.61169577, 0.5960095 ], [0.61960787, 0.6 , 0.5843084 ], [0.61960787, 0.6 , 0.5867053 ]], [[0.35487074, 0.30781192, 0.31565505], [0.3587888 , 0.31172997, 0.3195731 ], [0.35075194, 0.30400825, 0.303378 ], ..., [0.6684691 , 0.6488612 , 0.63317496], [0.6535535 , 0.63133717, 0.6273979 ], [0.65505916, 0.6320339 , 0.6305214 ]]], [[[0.32352942, 0.31176472, 0.29215688], [0.33953083, 0.32776612, 0.30815828], [0.34856445, 0.33679974, 0.3171919 ], ..., [0.3016457 , 0.28203785, 0.26635158], [0.30957633, 0.2899685 , 0.27428222], [0.30957633, 0.2899685 , 0.27428222]], [[0.3147759 , 0.3030112 , 0.28340337], [0.33077732, 0.3190126 , 0.29940477], [0.33981094, 0.32804623, 0.3084384 ], ..., [0.3010178 , 0.28140995, 0.26572368], [0.3051996 , 0.28559175, 0.26990548], [0.3051996 , 0.28559175, 0.26990548]], [[0.30716038, 0.29539567, 0.27578783], [0.32316178, 0.31139708, 0.29178923], [0.3321954 , 0.3204307 , 0.30082285], ..., [0.2979478 , 0.27833995, 0.26265368], [0.29968488, 0.28007704, 0.26439077], [0.29968488, 0.28007704, 0.26439077]], ..., [[0.38116246, 0.34194675, 0.29488793], [0.3828408 , 0.3436251 , 0.29656628], [0.36612785, 0.32691216, 0.27985334], ..., [0.4471094 , 0.4353447 , 0.41573685], [0.48121503, 0.46552876, 0.45376405], [0.48121503, 0.46552876, 0.45376405]], [[0.37689126, 0.33767554, 0.29061672], [0.37715337, 0.33793765, 0.29087883], [0.36428103, 0.32506534, 0.27800652], ..., [0.4166144 , 0.4048497 , 0.38524187], [0.45939228, 0.443706 , 0.4319413 ], [0.48061773, 0.46493146, 0.45316675]], [[0.37647063, 0.3372549 , 0.2901961 ], [0.37647063, 0.3372549 , 0.2901961 ], [0.36439076, 0.32517508, 0.27811626], ..., [0.3915626 , 0.3797979 , 0.36019006], [0.45455205, 0.43886578, 0.42710108], [0.48025194, 0.46456566, 0.45280096]]]], dtype=float32), array([[False, True], [False, True], [ True, False], [ True, False], [False, True], [ True, False], [False, True], [ True, False], [ True, False], [ True, False], [ True, False], [ True, False], [ True, False], [False, True], [ True, False], [ True, False], [ True, False], [ True, False], [False, True], [ True, False], [ True, False], [False, True], [False, True], [ True, False], [ True, False], [ True, False], [False, True], [ True, False], [ True, False], [False, True], [ True, False], [ True, False]])) . len(train_images), len(train_labels) . (32, 32) . train_images, train_labels = next(train_data.as_numpy_iterator()) show_images(train_images,train_labels) . val_images, val_labels = next(val_data.as_numpy_iterator()) show_images(val_images, val_labels) . Building a model . Before we build a model&gt; . The input shape. | The output shape. | The URL of the model we want to use. | . INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] # Batch, height, width, Colour_chanels # Setup output shape of the model OUTPUT_SHAPE = len(unique_category) # Setup model URL MODEL_URL = &quot;https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4&quot; . def create_model(input_shape=INPUT_SHAPE,output_shape=OUTPUT_SHAPE, model_url=MODEL_URL): print(&quot;Building model with:&quot;, model_url) # Setup the model model = tf.keras.Sequential([ hub.KerasLayer(model_url), tf.keras.layers.Dense(units=output_shape, activation=&quot;softmax&quot;) ]) # Compile the model model.compile( loss = tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;] ) # Build the model model.build(input_shape) return model . model = create_model() model.summary() . Building model with: https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4 Model: &#34;sequential_4&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer_4 (KerasLayer) (None, 1001) 5432713 _________________________________________________________________ dense_4 (Dense) (None, 2) 2004 ================================================================= Total params: 5,434,717 Trainable params: 2,004 Non-trainable params: 5,432,713 _________________________________________________________________ . Creating callbacks . TesnorBoard Callback . %load_ext tensorboard . import datetime import os # Create a function to build a TensorBoard callback def create_tensorboard_callback(): logdir = os.path.join(&quot;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/log&quot;, datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)) return tf.keras.callbacks.TensorBoard(logdir) . Early stopping callback . early_stopping = tf.keras.callbacks.EarlyStopping(monitor=&quot;val_accuracy&quot;, patience = 3) . Training a model . NUM_EPOCHS = 100 . print(&quot;GPU is on&quot; if tf.config.list_physical_devices(&quot;GPU&quot;) else &quot;No GPU :(&quot;) . GPU is on . def train_model(): tensorboard = create_tensorboard_callback() # Fit the model model.fit(x=train_data, epochs= NUM_EPOCHS, validation_data=val_data, validation_freq = 1, callbacks = [tensorboard,early_stopping]) return model . model = train_model() . Epoch 1/100 1/626 [..............................] - ETA: 0s - loss: 1.7851 - accuracy: 0.3438WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01. Instructions for updating: use `tf.profiler.experimental.stop` instead. . WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01. Instructions for updating: use `tf.profiler.experimental.stop` instead. . 626/626 [==============================] - 11646s 19s/step - loss: 0.0721 - accuracy: 0.9751 - val_loss: 0.0298 - val_accuracy: 0.9890 Epoch 2/100 626/626 [==============================] - 90s 144ms/step - loss: 0.0358 - accuracy: 0.9884 - val_loss: 0.0285 - val_accuracy: 0.9894 Epoch 3/100 626/626 [==============================] - 88s 140ms/step - loss: 0.0309 - accuracy: 0.9905 - val_loss: 0.0334 - val_accuracy: 0.9896 Epoch 4/100 626/626 [==============================] - 88s 140ms/step - loss: 0.0297 - accuracy: 0.9906 - val_loss: 0.0371 - val_accuracy: 0.9884 Epoch 5/100 626/626 [==============================] - 89s 143ms/step - loss: 0.0280 - accuracy: 0.9909 - val_loss: 0.0353 - val_accuracy: 0.9876 Epoch 6/100 626/626 [==============================] - 88s 141ms/step - loss: 0.0261 - accuracy: 0.9913 - val_loss: 0.0321 - val_accuracy: 0.9900 Epoch 7/100 626/626 [==============================] - 87s 138ms/step - loss: 0.0248 - accuracy: 0.9919 - val_loss: 0.0396 - val_accuracy: 0.9892 Epoch 8/100 626/626 [==============================] - 86s 137ms/step - loss: 0.0241 - accuracy: 0.9913 - val_loss: 0.0367 - val_accuracy: 0.9884 Epoch 9/100 626/626 [==============================] - 87s 139ms/step - loss: 0.0211 - accuracy: 0.9928 - val_loss: 0.0419 - val_accuracy: 0.9890 . %tensorboard --logdir /content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/log . Making and evaluating prediction using a trained model . predictions = model.predict(val_data, verbose=1) predictions . 157/157 [==============================] - 18s 114ms/step . array([[9.9999869e-01, 1.2679378e-06], [6.4851592e-06, 9.9999356e-01], [1.8989309e-05, 9.9998105e-01], ..., [1.0000000e+00, 1.2742490e-08], [1.0000000e+00, 1.9644544e-08], [1.0814135e-05, 9.9998915e-01]], dtype=float32) . predictions.shape . (5003, 2) . np.sum(predictions[0]) . 0.99999994 . def make_prediction(prediction_probabilities): &quot;&quot;&quot; Turns an array of prediction probabilities into a label. &quot;&quot;&quot; return unique_category[np.argmax(prediction_probabilities)] . pred_label = make_prediction(predictions[17]) pred_label . 0 . val_data . &lt;BatchDataset shapes: ((None, 224, 224, 3), (None, 2)), types: (tf.float32, tf.bool)&gt; . def unbatchify(data): &quot;&quot;&quot; Takes a batched dataset of (image, label) Tensors and reutrns separate arrays of images and labels. &quot;&quot;&quot; images = [] labels = [] # Loop through unbatched data for image, label in data.unbatch().as_numpy_iterator(): images.append(image) labels.append(unique_category[np.argmax(label)]) return images, labels # Unbatchify the validation data val_images, val_labels = unbatchify(val_data) val_images[0], val_labels[0] . (array([[[0.3839508 , 0.46238217, 0.6074802 ], [0.3810609 , 0.45949227, 0.6045903 ], [0.37647063, 0.454902 , 0.6 ], ..., [0.21960786, 0.28788537, 0.40553245], [0.21960786, 0.28627452, 0.4039216 ], [0.21960786, 0.28627452, 0.4039216 ]], [[0.39075977, 0.46919113, 0.61428916], [0.38780484, 0.4662362 , 0.61133426], [0.38371852, 0.4621499 , 0.60724795], ..., [0.22029103, 0.28856856, 0.40621564], [0.21960786, 0.28627452, 0.4039216 ], [0.21960786, 0.28627452, 0.4039216 ]], [[0.39936978, 0.47780114, 0.6228992 ], [0.3961657 , 0.47459707, 0.61969507], [0.39247203, 0.4709034 , 0.6160014 ], ..., [0.220478 , 0.28875554, 0.40640262], [0.21960786, 0.28627452, 0.4039216 ], [0.21960786, 0.28627452, 0.4039216 ]], ..., [[0.4504261 , 0.43473983, 0.3988155 ], [0.4474612 , 0.4317749 , 0.39630637], [0.4474387 , 0.4317524 , 0.3964583 ], ..., [0.7306667 , 0.7157211 , 0.68042696], [0.7375876 , 0.7186452 , 0.68443644], [0.7516807 , 0.72422963, 0.6928571 ]], [[0.44408268, 0.43231797, 0.41110635], [0.45273113, 0.44096643, 0.41951722], [0.4501751 , 0.4384104 , 0.4135161 ], ..., [0.7335602 , 0.7217955 , 0.70082134], [0.73665965, 0.72489494, 0.7004757 ], [0.73665965, 0.72489494, 0.70023835]], [[0.44408268, 0.43231797, 0.41271013], [0.45273113, 0.44096643, 0.4213586 ], [0.4501751 , 0.4384104 , 0.41880256], ..., [0.7254081 , 0.7136434 , 0.69403553], [0.7264005 , 0.7146358 , 0.69502795], [0.7264005 , 0.7146358 , 0.69502795]]], dtype=float32), 0) . def plot_pred(prediction_probabilities, labels, images, n=1): &quot;&quot;&quot; View the prediction, ground truth and image for sample n &quot;&quot;&quot; pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n] # Get the pred label pred_label = make_prediction(pred_prob) # Plot image &amp; remove ticks plt.imshow(image) plt.xticks([]) plt.yticks([]) # Change the colour of the title depending on if the prediction is right or wrong if pred_label == true_label: color = &quot;green&quot; else: color = &quot;red&quot; # Change plot title to be predicted, probability of prediction and truth label plt.title(&quot;{} {:2.0f}% {}&quot;.format(pred_label, np.max(pred_prob)*100, true_label), color=color) . plot_pred(prediction_probabilities=predictions, labels=val_labels, images=val_images, n=15) . def plot_pred_conf(prediction_probabilities, labels, n=1): &quot;&quot;&quot; Plus the top 10 highest prediction confidences along with the truth label for sample n. &quot;&quot;&quot; pred_prob, true_label = prediction_probabilities[n], labels[n] # Get the predicted label pred_label = make_prediction(pred_prob) # Find the top 10 prediction confidence indexes top_10_pred_indexes = pred_prob.argsort()[-10:][::-1] # Find the top 10 prediction confidence values top_10_pred_values = pred_prob[top_10_pred_indexes] # Find the top 10 prediction labels top_10_pred_labels = unique_category[top_10_pred_indexes] # Setup plot top_plot = plt.bar(np.arange(len(top_10_pred_labels)), top_10_pred_values, color=&quot;grey&quot;) plt.xticks(np.arange(len(top_10_pred_labels)), labels=top_10_pred_labels, rotation=&quot;vertical&quot;) # Change color of true label if np.isin(true_label, top_10_pred_labels): top_plot[np.argmax(top_10_pred_labels == true_label)].set_color(&quot;green&quot;) else: pass . plot_pred_conf(prediction_probabilities=predictions, labels=val_labels, n=9) . i_multiplier = 20 num_rows = 5 num_cols = 5 num_images = num_rows*num_cols plt.figure(figsize=(10*num_cols, 5*num_rows)) for i in range(num_images): plt.subplot(num_rows, 2*num_cols, 2*i+1) plot_pred(prediction_probabilities=predictions, labels=val_labels, images=val_images, n=i+i_multiplier) plt.subplot(num_rows, 2*num_cols, 2*i+2) plot_pred_conf(prediction_probabilities=predictions, labels=val_labels, n=i+i_multiplier) plt.tight_layout(h_pad=1.0) plt.show() . Saving and reloading a trained model . def save_model(model, suffix=None): &quot;&quot;&quot; Saves a given model in a models directory and appends a suffix (string). &quot;&quot;&quot; # Create a model directory pathname with current time modeldir = &quot;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/&quot; model_path = modeldir + &quot;-&quot; + suffix + &quot;.h5&quot; # save format of model print(f&quot;Saving model to: {model_path}...&quot;) model.save(model_path) return model_path . save_model(model, suffix=&quot;mobilenetv2-Adam&quot;) . Saving model to: /content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/-mobilenetv2-Adam.h5... . &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/-mobilenetv2-Adam.h5&#39; . model.evaluate(val_data) . 157/157 [==============================] - 18s 116ms/step - loss: 0.0419 - accuracy: 0.9890 . [0.0419415719807148, 0.9890065789222717] . Predict on custom data . test = &#39;/content/drive/MyDrive/dog.jpg&#39; test = imread(test) test = tf.image.convert_image_dtype(test,tf.float32) test = tf.image.resize(test, size=[224,224]) test = np.expand_dims(test,axis=0) . test.shape . (1, 224, 224, 3) . pred = model.predict(test) pred . array([[0.01689671, 0.9831033 ]], dtype=float32) . pred_label = make_prediction(pred) pred_label . 1 . test1 = &#39;/content/drive/MyDrive/cat.jpg&#39; test1 = imread(test1) test1 = tf.image.convert_image_dtype(test1,tf.float32) test1 = tf.image.resize(test1, size=[224,224]) test1 = np.expand_dims(test1,axis=0) . pred1 = model.predict(test1) pred1 . array([[1.0000000e+00, 6.8222934e-09]], dtype=float32) . pred_label = make_prediction(pred1) pred_label . 0 . Testing Fun . def test_data(path): demo = imread(path) demo = tf.image.convert_image_dtype(demo,tf.float32) demo = tf.image.resize(demo,size=[224,224]) demo = np.expand_dims(demo,axis=0) pred = model.predict(demo) result = unique_category[np.argmax(pred)] return result . path = &#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/test/cat.jpg&#39; path1 =&#39;/content/drive/MyDrive/COLAB_PROJECT/1.CNN/Dog_vs_cat/test/dog.jpg&#39; . test_data(path) . 0 . test_data(path1) . 1 .",
            "url": "https://vivek2509.github.io/World_of_ML/deep%20learning/cnn/2020/11/21/DOG_VS_CAT.html",
            "relUrl": "/deep%20learning/cnn/2020/11/21/DOG_VS_CAT.html",
            "date": " • Nov 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Random Forest Regression",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . housing = pd.read_csv(&quot;housing.csv&quot;) housing . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 20635 -121.09 | 39.48 | 25.0 | 1665.0 | 374.0 | 845.0 | 330.0 | 1.5603 | 78100.0 | INLAND | . 20636 -121.21 | 39.49 | 18.0 | 697.0 | 150.0 | 356.0 | 114.0 | 2.5568 | 77100.0 | INLAND | . 20637 -121.22 | 39.43 | 17.0 | 2254.0 | 485.0 | 1007.0 | 433.0 | 1.7000 | 92300.0 | INLAND | . 20638 -121.32 | 39.43 | 18.0 | 1860.0 | 409.0 | 741.0 | 349.0 | 1.8672 | 84700.0 | INLAND | . 20639 -121.24 | 39.37 | 16.0 | 2785.0 | 616.0 | 1387.0 | 530.0 | 2.3886 | 89400.0 | INLAND | . 20640 rows × 10 columns . 0.3 Check if any null value . housing.isna().sum() . longitude 0 latitude 0 housing_median_age 0 total_rooms 0 total_bedrooms 207 population 0 households 0 median_income 0 median_house_value 0 ocean_proximity 0 dtype: int64 . housing[&#39;total_bedrooms&#39;].median() . 435.0 . housing[&#39;total_bedrooms&#39;].fillna(housing[&#39;total_bedrooms&#39;].median(),inplace=True) #with pandas fillna . housing.isna().sum() . longitude 0 latitude 0 housing_median_age 0 total_rooms 0 total_bedrooms 0 population 0 households 0 median_income 0 median_house_value 0 ocean_proximity 0 dtype: int64 . housing.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 longitude 20640 non-null float64 1 latitude 20640 non-null float64 2 housing_median_age 20640 non-null float64 3 total_rooms 20640 non-null float64 4 total_bedrooms 20640 non-null float64 5 population 20640 non-null float64 6 households 20640 non-null float64 7 median_income 20640 non-null float64 8 median_house_value 20640 non-null float64 9 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB . 0.4 Split into X &amp; y . X = housing.drop(&quot;median_house_value&quot;,axis=1) X . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | NEAR BAY | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | NEAR BAY | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | NEAR BAY | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | NEAR BAY | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | NEAR BAY | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 20635 -121.09 | 39.48 | 25.0 | 1665.0 | 374.0 | 845.0 | 330.0 | 1.5603 | INLAND | . 20636 -121.21 | 39.49 | 18.0 | 697.0 | 150.0 | 356.0 | 114.0 | 2.5568 | INLAND | . 20637 -121.22 | 39.43 | 17.0 | 2254.0 | 485.0 | 1007.0 | 433.0 | 1.7000 | INLAND | . 20638 -121.32 | 39.43 | 18.0 | 1860.0 | 409.0 | 741.0 | 349.0 | 1.8672 | INLAND | . 20639 -121.24 | 39.37 | 16.0 | 2785.0 | 616.0 | 1387.0 | 530.0 | 2.3886 | INLAND | . 20640 rows × 9 columns . y = housing[&quot;median_house_value&quot;] y . 0 452600.0 1 358500.0 2 352100.0 3 341300.0 4 342200.0 ... 20635 78100.0 20636 77100.0 20637 92300.0 20638 84700.0 20639 89400.0 Name: median_house_value, Length: 20640, dtype: float64 . 0.5 Convert categorical data into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_features = [&quot;ocean_proximity&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_features)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) . pd.DataFrame(transformed_X) . 0 1 2 3 4 5 6 7 8 9 10 11 12 . 0 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | . 1 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | . 2 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | . 3 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | . 4 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 20635 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | -121.09 | 39.48 | 25.0 | 1665.0 | 374.0 | 845.0 | 330.0 | 1.5603 | . 20636 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | -121.21 | 39.49 | 18.0 | 697.0 | 150.0 | 356.0 | 114.0 | 2.5568 | . 20637 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | -121.22 | 39.43 | 17.0 | 2254.0 | 485.0 | 1007.0 | 433.0 | 1.7000 | . 20638 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | -121.32 | 39.43 | 18.0 | 1860.0 | 409.0 | 741.0 | 349.0 | 1.8672 | . 20639 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | -121.24 | 39.37 | 16.0 | 2785.0 | 616.0 | 1387.0 | 530.0 | 2.3886 | . 20640 rows × 13 columns . 0.6 Split the data into test and train . from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(transformed_X, y, test_size = 0.25, random_state = 2509) . 1. Training the Random Forest Regression model on the training set . from sklearn.ensemble import RandomForestRegressor model = RandomForestRegressor() model.fit(x_train, y_train) . RandomForestRegressor() . 1.2 Score . model.score(x_test, y_test) . 0.8214712477743407 . 2. Predicting a new result on test set . y_preds = model.predict(x_test) . df = pd.DataFrame(data={&quot;actual values&quot;: y_test, &quot;predicted values&quot;: y_preds}) df[&quot;differences&quot;] = df[&quot;predicted values&quot;] - df[&quot;actual values&quot;] df . actual values predicted values differences . 14352 225000.0 | 213052.00 | -11948.00 | . 13882 99300.0 | 109818.00 | 10518.00 | . 4223 230600.0 | 250500.00 | 19900.00 | . 2428 55100.0 | 58660.00 | 3560.00 | . 18402 274100.0 | 330573.04 | 56473.04 | . ... ... | ... | ... | . 11691 217000.0 | 179878.00 | -37122.00 | . 1213 79900.0 | 95071.00 | 15171.00 | . 15957 246200.0 | 232699.01 | -13500.99 | . 13982 53300.0 | 84761.00 | 31461.00 | . 11212 182900.0 | 189426.00 | 6526.00 | . 5160 rows × 3 columns . 3. Save a model . import pickle # Save an extisting model to file pickle.dump(model, open(&quot;random_forest_model.pkl&quot;, &quot;wb&quot;)) .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/regression/2020/10/27/Random_forest_regression.html",
            "relUrl": "/jupyter/regression/2020/10/27/Random_forest_regression.html",
            "date": " • Oct 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Random forest classification",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&quot;heart-disease.csv&quot;) dataset . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 298 57 | 0 | 0 | 140 | 241 | 0 | 1 | 123 | 1 | 0.2 | 1 | 0 | 3 | 0 | . 299 45 | 1 | 3 | 110 | 264 | 0 | 1 | 132 | 0 | 1.2 | 1 | 0 | 3 | 0 | . 300 68 | 1 | 0 | 144 | 193 | 1 | 1 | 141 | 0 | 3.4 | 1 | 2 | 3 | 0 | . 301 57 | 1 | 0 | 130 | 131 | 0 | 1 | 115 | 1 | 1.2 | 1 | 1 | 3 | 0 | . 302 57 | 0 | 1 | 130 | 236 | 0 | 0 | 174 | 0 | 0.0 | 1 | 1 | 2 | 0 | . 303 rows × 14 columns . 0.3 Check if any null value . dataset.isna().sum() . age 0 sex 0 cp 0 trestbps 0 chol 0 fbs 0 restecg 0 thalach 0 exang 0 oldpeak 0 slope 0 ca 0 thal 0 target 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 303 entries, 0 to 302 Data columns (total 14 columns): # Column Non-Null Count Dtype -- -- 0 age 303 non-null int64 1 sex 303 non-null int64 2 cp 303 non-null int64 3 trestbps 303 non-null int64 4 chol 303 non-null int64 5 fbs 303 non-null int64 6 restecg 303 non-null int64 7 thalach 303 non-null int64 8 exang 303 non-null int64 9 oldpeak 303 non-null float64 10 slope 303 non-null int64 11 ca 303 non-null int64 12 thal 303 non-null int64 13 target 303 non-null int64 dtypes: float64(1), int64(13) memory usage: 33.3 KB . 0.4 Split into X &amp; y . X = dataset.drop(&quot;target&quot;, axis=1) X.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | . y = dataset[&quot;target&quot;] y.head() . 0 1 1 1 2 1 3 1 4 1 Name: target, dtype: int64 . 0.6 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 2509) . 1.Training the model on the Training set . from sklearn.ensemble import RandomForestClassifier classifier = RandomForestClassifier(random_state=2509) classifier.fit(X_train, y_train) . RandomForestClassifier(random_state=2509) . 1.1 Score . classifier.score(X_test,y_test) . 0.8552631578947368 . 2.Predicting the Test set results . y_pred = classifier.predict(X_test) . 2.2 Making the Confusion Matrix . from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_test, y_pred) print(cm) . [[28 5] [ 6 37]] . from sklearn.metrics import classification_report print(classification_report(y_test, y_pred)) . precision recall f1-score support 0 0.82 0.85 0.84 33 1 0.88 0.86 0.87 43 accuracy 0.86 76 macro avg 0.85 0.85 0.85 76 weighted avg 0.86 0.86 0.86 76 .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/classification/2020/10/27/Random_forest_classification.html",
            "relUrl": "/jupyter/classification/2020/10/27/Random_forest_classification.html",
            "date": " • Oct 27, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "decision tree regression",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Position_Salaries.csv&#39;) dataset . Position Level Salary . 0 Business Analyst | 1 | 45000 | . 1 Junior Consultant | 2 | 50000 | . 2 Senior Consultant | 3 | 60000 | . 3 Manager | 4 | 80000 | . 4 Country Manager | 5 | 110000 | . 5 Region Manager | 6 | 150000 | . 6 Partner | 7 | 200000 | . 7 Senior Partner | 8 | 300000 | . 8 C-level | 9 | 500000 | . 9 CEO | 10 | 1000000 | . 0.3 Split into X &amp; y . X = dataset.iloc[:, 1:-1].values y = dataset.iloc[:, -1].values . 1. Training the decision tree regression model on the whole dataset . from sklearn.tree import DecisionTreeRegressor regressor = DecisionTreeRegressor() regressor.fit(X, y) . DecisionTreeRegressor() . 2. Predicting a new result with Linear Regression . regressor.predict([[6.5]]) . array([150000.]) . 3. Visualising the Decision Tree Regression results (higher resolution) . X_grid = np.arange(min(X), max(X), 0.01) X_grid = X_grid.reshape((len(X_grid), 1)) plt.scatter(X, y, color = &#39;red&#39;) plt.plot(X_grid, regressor.predict(X_grid), color = &#39;blue&#39;) plt.title(&#39;Truth or Bluff (Decision Tree Regression)&#39;) plt.xlabel(&#39;Position level&#39;) plt.ylabel(&#39;Salary&#39;) plt.show() .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/regression/2020/10/26/Decision_tree_regression.html",
            "relUrl": "/jupyter/regression/2020/10/26/Decision_tree_regression.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Decision Tree Classification",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Social_Network_Ads.csv&#39;) dataset . User ID Gender Age EstimatedSalary Purchased . 0 15624510 | Male | 19 | 19000 | 0 | . 1 15810944 | Male | 35 | 20000 | 0 | . 2 15668575 | Female | 26 | 43000 | 0 | . 3 15603246 | Female | 27 | 57000 | 0 | . 4 15804002 | Male | 19 | 76000 | 0 | . ... ... | ... | ... | ... | ... | . 395 15691863 | Female | 46 | 41000 | 1 | . 396 15706071 | Male | 51 | 23000 | 1 | . 397 15654296 | Female | 50 | 20000 | 1 | . 398 15755018 | Male | 36 | 33000 | 0 | . 399 15594041 | Female | 49 | 36000 | 1 | . 400 rows × 5 columns . 0.3 Check if any null value . dataset.isna().sum() . User ID 0 Gender 0 Age 0 EstimatedSalary 0 Purchased 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 400 entries, 0 to 399 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 User ID 400 non-null int64 1 Gender 400 non-null object 2 Age 400 non-null int64 3 EstimatedSalary 400 non-null int64 4 Purchased 400 non-null int64 dtypes: int64(4), object(1) memory usage: 15.8+ KB . Drop User ID . dataset.drop(&#39;User ID&#39;, axis=1, inplace=True) dataset.head() . Gender Age EstimatedSalary Purchased . 0 Male | 19 | 19000 | 0 | . 1 Male | 35 | 20000 | 0 | . 2 Female | 26 | 43000 | 0 | . 3 Female | 27 | 57000 | 0 | . 4 Male | 19 | 76000 | 0 | . 0.4 Split into X &amp; y . X = dataset.drop(&#39;Purchased&#39;, axis=1) X.head() . Gender Age EstimatedSalary . 0 Male | 19 | 19000 | . 1 Male | 35 | 20000 | . 2 Female | 26 | 43000 | . 3 Female | 27 | 57000 | . 4 Male | 19 | 76000 | . y = dataset[&#39;Purchased&#39;] y.head() . 0 0 1 0 2 0 3 0 4 0 Name: Purchased, dtype: int64 . 0.5 Convert categories into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_feature = [&quot;Gender&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_feature)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) . pd.DataFrame(transformed_X).head() . 0 1 2 3 . 0 0.0 | 1.0 | 19.0 | 19000.0 | . 1 0.0 | 1.0 | 35.0 | 20000.0 | . 2 1.0 | 0.0 | 26.0 | 43000.0 | . 3 1.0 | 0.0 | 27.0 | 57000.0 | . 4 0.0 | 1.0 | 19.0 | 76000.0 | . 0.6 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size = 0.25, random_state = 2509) . 0.7 Feature Scaling . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) . 1.Training the model on the Training set . from sklearn.tree import DecisionTreeClassifier classifier = DecisionTreeClassifier(criterion=&#39;entropy&#39;) classifier.fit(X_train, y_train) . DecisionTreeClassifier(criterion=&#39;entropy&#39;) . 1.1 Score . classifier.score(X_test,y_test) . 0.86 . 2.Predicting the Test set results . y_pred = classifier.predict(X_test) . 2.2 Making the Confusion Matrix . from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_test, y_pred) print(cm) . [[61 9] [ 5 25]] .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/classification/2020/10/26/Decision_tree_classification.html",
            "relUrl": "/jupyter/classification/2020/10/26/Decision_tree_classification.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Naive bayes",
            "content": "0. Data preprocessing . 0.1 Import library . import pandas as pd import numpy as np import matplotlib.pyplot as plt . 0.2 Import dataset . email = pd.read_csv(&#39;emails.csv&#39;) email . text spam . 0 Subject: naturally irresistible your corporate... | 1 | . 1 Subject: the stock trading gunslinger fanny i... | 1 | . 2 Subject: unbelievable new homes made easy im ... | 1 | . 3 Subject: 4 color printing special request add... | 1 | . 4 Subject: do not have money , get software cds ... | 1 | . ... ... | ... | . 5723 Subject: re : research and development charges... | 0 | . 5724 Subject: re : receipts from visit jim , than... | 0 | . 5725 Subject: re : enron case study update wow ! a... | 0 | . 5726 Subject: re : interest david , please , call... | 0 | . 5727 Subject: news : aurora 5 . 2 update aurora ve... | 0 | . 5728 rows × 2 columns . email.head() . text spam . 0 Subject: naturally irresistible your corporate... | 1 | . 1 Subject: the stock trading gunslinger fanny i... | 1 | . 2 Subject: unbelievable new homes made easy im ... | 1 | . 3 Subject: 4 color printing special request add... | 1 | . 4 Subject: do not have money , get software cds ... | 1 | . len(email) . 5728 . 0.3 Check if any null value . email.isna().sum() . text 0 spam 0 dtype: int64 . email.shape . (5728, 2) . email.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5728 entries, 0 to 5727 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 text 5728 non-null object 1 spam 5728 non-null int64 dtypes: int64(1), object(1) memory usage: 89.6+ KB . 0.4 Cleaning the text . import re import nltk nltk.download(&#39;stopwords&#39;) from nltk.corpus import stopwords from nltk.stem.porter import PorterStemmer corpus = [] for i in range(0, len(email)): e_mail = re.sub(&#39;[^a-zA-Z]&#39;, &#39; &#39;, email[&#39;text&#39;][i]) e_mail = e_mail.split() ps = PorterStemmer() e_mail = [ps.stem(word) for word in e_mail if not word in set(stopwords.words(&#39;english&#39;))] e_mail = &#39; &#39;.join(e_mail) corpus.append(e_mail) . [nltk_data] Downloading package stopwords to [nltk_data] C: Users patel AppData Roaming nltk_data... [nltk_data] Package stopwords is already up-to-date! . corpus[2509] . &#39;subject enron mid year perform manag process enron mid year perform manag process begun process requir select suggest review provid perform relat feedback may also request provid feedback fellow employe need access perform manag system pep http pep enron com question direct pep help desk follow number u option europ option canada canada employe e mail question perfmgmt enron com log pep enter user id password provid log immedi prompt chang secur password user id password user id wkamin password welcom&#39; . 0.5 Creating the Bag of Words model . from sklearn.feature_extraction.text import CountVectorizer cv = CountVectorizer() X = cv.fit_transform(corpus).toarray() y = email[&#39;spam&#39;] . len(X) . 5728 . X.shape . (5728, 25607) . len(y) . 5728 . 0.6 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 2509) . 1. Training the Naive Bayes model on the Training set . 1.1.1 GaussianNB . from sklearn.naive_bayes import GaussianNB GN_classifier = GaussianNB() GN_classifier.fit(X_train, y_train) . GaussianNB() . 1.1.2 Score . GN_score = GN_classifier.score(X_test,y_test) GN_score . 0.9528795811518325 . y_GN_pred = GN_classifier.predict(X_test) #Predicting the Test set results . 1.2.1 MultinomialNB . from sklearn.naive_bayes import MultinomialNB MN_classifier = MultinomialNB() MN_classifier.fit(X_train, y_train) . MultinomialNB() . 1.2.2 Score . MN_score = MN_classifier.score(X_test,y_test) MN_score . 0.9842931937172775 . y_MN_pred = MN_classifier.predict(X_test) #Predicting the Test set results . 2. Making the Confusion Matrix . 2.1 GaussianNB . from sklearn.metrics import confusion_matrix GN_cm = confusion_matrix(y_test, y_GN_pred) print(GN_cm) . [[849 26] [ 28 243]] . 2.2 MultinomialNB . from sklearn.metrics import confusion_matrix MN_cm = confusion_matrix(y_test, y_MN_pred) print(MN_cm) . [[862 13] [ 5 266]] . 3. Compare Both models . models = pd.DataFrame({&quot;GaussianNB&quot;: GN_score, &quot;MultinomialNB&quot;: MN_score }, index=[0]) models.T.plot.bar(title=&quot;Comapre different models&quot;, legend=False) plt.xticks(rotation=0); . 4. SAVE MODEL . import pickle pickle.dump(MN_classifier,open(&quot;Email_spam_naive_bayes_MN.pkl&quot;,&quot;wb&quot;)) .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/classification/2020/10/25/Naive-Bayes.html",
            "relUrl": "/jupyter/classification/2020/10/25/Naive-Bayes.html",
            "date": " • Oct 25, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Kernel SVM",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Social_Network_Ads.csv&#39;) dataset . User ID Gender Age EstimatedSalary Purchased . 0 15624510 | Male | 19 | 19000 | 0 | . 1 15810944 | Male | 35 | 20000 | 0 | . 2 15668575 | Female | 26 | 43000 | 0 | . 3 15603246 | Female | 27 | 57000 | 0 | . 4 15804002 | Male | 19 | 76000 | 0 | . ... ... | ... | ... | ... | ... | . 395 15691863 | Female | 46 | 41000 | 1 | . 396 15706071 | Male | 51 | 23000 | 1 | . 397 15654296 | Female | 50 | 20000 | 1 | . 398 15755018 | Male | 36 | 33000 | 0 | . 399 15594041 | Female | 49 | 36000 | 1 | . 400 rows × 5 columns . 0.3 Check if any null value . dataset.isna().sum() . User ID 0 Gender 0 Age 0 EstimatedSalary 0 Purchased 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 400 entries, 0 to 399 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 User ID 400 non-null int64 1 Gender 400 non-null object 2 Age 400 non-null int64 3 EstimatedSalary 400 non-null int64 4 Purchased 400 non-null int64 dtypes: int64(4), object(1) memory usage: 15.8+ KB . Drop User ID . dataset.drop(&#39;User ID&#39;, axis=1, inplace=True) dataset.head() . Gender Age EstimatedSalary Purchased . 0 Male | 19 | 19000 | 0 | . 1 Male | 35 | 20000 | 0 | . 2 Female | 26 | 43000 | 0 | . 3 Female | 27 | 57000 | 0 | . 4 Male | 19 | 76000 | 0 | . 0.4 Split into X &amp; y . X = dataset.drop(&#39;Purchased&#39;, axis=1) X.head() . Gender Age EstimatedSalary . 0 Male | 19 | 19000 | . 1 Male | 35 | 20000 | . 2 Female | 26 | 43000 | . 3 Female | 27 | 57000 | . 4 Male | 19 | 76000 | . y = dataset[&#39;Purchased&#39;] y.head() . 0 0 1 0 2 0 3 0 4 0 Name: Purchased, dtype: int64 . 0.5 Convert categories into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_feature = [&quot;Gender&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_feature)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) . pd.DataFrame(transformed_X).head() . 0 1 2 3 . 0 0.0 | 1.0 | 19.0 | 19000.0 | . 1 0.0 | 1.0 | 35.0 | 20000.0 | . 2 1.0 | 0.0 | 26.0 | 43000.0 | . 3 1.0 | 0.0 | 27.0 | 57000.0 | . 4 0.0 | 1.0 | 19.0 | 76000.0 | . 0.6 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size = 0.25, random_state = 2509) . 0.7 Feature Scaling . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) . 1.Training the model on the Training set . from sklearn.svm import SVC classifier = SVC(kernel = &#39;rbf&#39;) classifier.fit(X_train, y_train) . SVC() . 1.1 Score . classifier.score(X_test,y_test) . 0.92 . 2.Predicting the Test set results . y_pred = classifier.predict(X_test) . 2.2 Making the Confusion Matrix . from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_test, y_pred) print(cm) . [[65 5] [ 3 27]] .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/classification/2020/10/24/kernel_svm.html",
            "relUrl": "/jupyter/classification/2020/10/24/kernel_svm.html",
            "date": " • Oct 24, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Support Vector Regression (SVR)",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Position_Salaries.csv&#39;) dataset . Position Level Salary . 0 Business Analyst | 1 | 45000 | . 1 Junior Consultant | 2 | 50000 | . 2 Senior Consultant | 3 | 60000 | . 3 Manager | 4 | 80000 | . 4 Country Manager | 5 | 110000 | . 5 Region Manager | 6 | 150000 | . 6 Partner | 7 | 200000 | . 7 Senior Partner | 8 | 300000 | . 8 C-level | 9 | 500000 | . 9 CEO | 10 | 1000000 | . 0.3 Split into X &amp; y . X = dataset.iloc[:, 1:-1].values y = dataset.iloc[:, -1].values . y = y.reshape(len(y),1) . 0.4 Feature Scaling . from sklearn.preprocessing import StandardScaler sc_X = StandardScaler() sc_y = StandardScaler() X = sc_X.fit_transform(X) y = sc_y.fit_transform(y) X . array([[-1.5666989 ], [-1.21854359], [-0.87038828], [-0.52223297], [-0.17407766], [ 0.17407766], [ 0.52223297], [ 0.87038828], [ 1.21854359], [ 1.5666989 ]]) . 1. Training the model on the whole dataset . from sklearn.svm import SVR regressor = SVR(kernel = &#39;rbf&#39;) regressor.fit(X, y) . C: VIVEK 1.PYTHON_DEV env tensor lib site-packages sklearn utils validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) . SVR() . 2. Predicting a new result with SVR . sc_y.inverse_transform(regressor.predict(sc_X.transform([[6.5]]))) . array([170370.0204065]) . 3. Visualising the SVR results . X_grid = np.arange(min(sc_X.inverse_transform(X)), max(sc_X.inverse_transform(X)), 0.1) X_grid = X_grid.reshape((len(X_grid), 1)) plt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color = &#39;red&#39;) plt.plot(X_grid, sc_y.inverse_transform(regressor.predict(sc_X.transform(X_grid))), color = &#39;blue&#39;) plt.title(&#39;SVR&#39;) plt.xlabel(&#39;Position level&#39;) plt.ylabel(&#39;Salary&#39;) plt.show() .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/regression/2020/10/24/Support-Vector-Regression-(SVR).html",
            "relUrl": "/jupyter/regression/2020/10/24/Support-Vector-Regression-(SVR).html",
            "date": " • Oct 24, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Polynomial Regression",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Position_Salaries.csv&#39;) dataset . Position Level Salary . 0 Business Analyst | 1 | 45000 | . 1 Junior Consultant | 2 | 50000 | . 2 Senior Consultant | 3 | 60000 | . 3 Manager | 4 | 80000 | . 4 Country Manager | 5 | 110000 | . 5 Region Manager | 6 | 150000 | . 6 Partner | 7 | 200000 | . 7 Senior Partner | 8 | 300000 | . 8 C-level | 9 | 500000 | . 9 CEO | 10 | 1000000 | . 0.3 Split into X &amp; y . X = dataset.iloc[:, 1:-1].values y = dataset.iloc[:, -1].values . Training the Linear Regression model on the whole dataset . from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X, y) . LinearRegression() . Training the Polynomial Regression model on the whole dataset . from sklearn.preprocessing import PolynomialFeatures poly_reg = PolynomialFeatures(degree = 4) X_poly = poly_reg.fit_transform(X) lin_reg_2 = LinearRegression() lin_reg_2.fit(X_poly, y) . LinearRegression() . Visualising the Linear Regression results . plt.scatter(X, y, color = &#39;red&#39;) plt.plot(X, lin_reg.predict(X), color = &#39;blue&#39;) plt.title(&#39;Truth or Bluff (Linear Regression)&#39;) plt.xlabel(&#39;Position Level&#39;) plt.ylabel(&#39;Salary&#39;) plt.show() . Visualising the Polynomial Regression results . plt.scatter(X, y, color = &#39;red&#39;) plt.plot(X, lin_reg_2.predict(poly_reg.fit_transform(X)), color = &#39;blue&#39;) plt.title(&#39;Truth or Bluff (Polynomial Regression)&#39;) plt.xlabel(&#39;Position level&#39;) plt.ylabel(&#39;Salary&#39;) plt.show() . Predicting a new result with Linear Regression . lin_reg.predict([[6.5]]) . array([330378.78787879]) . Predicting a new result with Polynomial Regression . lin_reg_2.predict(poly_reg.fit_transform([[6.5]])) . array([158862.45265153]) .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/regression/2020/10/23/polynomial_regression.html",
            "relUrl": "/jupyter/regression/2020/10/23/polynomial_regression.html",
            "date": " • Oct 23, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Supprt Vector Machine (SVM)",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Social_Network_Ads.csv&#39;) dataset . User ID Gender Age EstimatedSalary Purchased . 0 15624510 | Male | 19 | 19000 | 0 | . 1 15810944 | Male | 35 | 20000 | 0 | . 2 15668575 | Female | 26 | 43000 | 0 | . 3 15603246 | Female | 27 | 57000 | 0 | . 4 15804002 | Male | 19 | 76000 | 0 | . ... ... | ... | ... | ... | ... | . 395 15691863 | Female | 46 | 41000 | 1 | . 396 15706071 | Male | 51 | 23000 | 1 | . 397 15654296 | Female | 50 | 20000 | 1 | . 398 15755018 | Male | 36 | 33000 | 0 | . 399 15594041 | Female | 49 | 36000 | 1 | . 400 rows × 5 columns . 0.3 Check if any null value . dataset.isna().sum() . User ID 0 Gender 0 Age 0 EstimatedSalary 0 Purchased 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 400 entries, 0 to 399 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 User ID 400 non-null int64 1 Gender 400 non-null object 2 Age 400 non-null int64 3 EstimatedSalary 400 non-null int64 4 Purchased 400 non-null int64 dtypes: int64(4), object(1) memory usage: 15.8+ KB . Drop User ID . dataset.drop(&#39;User ID&#39;, axis=1, inplace=True) dataset.head() . Gender Age EstimatedSalary Purchased . 0 Male | 19 | 19000 | 0 | . 1 Male | 35 | 20000 | 0 | . 2 Female | 26 | 43000 | 0 | . 3 Female | 27 | 57000 | 0 | . 4 Male | 19 | 76000 | 0 | . 0.4 Split into X &amp; y . X = dataset.drop(&#39;Purchased&#39;, axis=1) X.head() . Gender Age EstimatedSalary . 0 Male | 19 | 19000 | . 1 Male | 35 | 20000 | . 2 Female | 26 | 43000 | . 3 Female | 27 | 57000 | . 4 Male | 19 | 76000 | . y = dataset[&#39;Purchased&#39;] y.head() . 0 0 1 0 2 0 3 0 4 0 Name: Purchased, dtype: int64 . 0.5 Convert categories into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_feature = [&quot;Gender&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_feature)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) . pd.DataFrame(transformed_X).head() . 0 1 2 3 . 0 0.0 | 1.0 | 19.0 | 19000.0 | . 1 0.0 | 1.0 | 35.0 | 20000.0 | . 2 1.0 | 0.0 | 26.0 | 43000.0 | . 3 1.0 | 0.0 | 27.0 | 57000.0 | . 4 0.0 | 1.0 | 19.0 | 76000.0 | . 0.6 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size = 0.25, random_state = 2509) . 0.7 Feature Scaling . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) . 1.Training the model on the Training set . from sklearn.svm import SVC classifier = SVC(kernel = &#39;linear&#39;, random_state = 0) classifier.fit(X_train, y_train) . SVC(kernel=&#39;linear&#39;, random_state=0) . 1.1 Score . classifier.score(X_test,y_test) . 0.88 . 2.Predicting the Test set results . y_pred = classifier.predict(X_test) . 2.2 Making the Confusion Matrix . from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_test, y_pred) print(cm) . [[65 5] [ 7 23]] .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/classification/2020/10/23/Supprt_Vector_Machine.html",
            "relUrl": "/jupyter/classification/2020/10/23/Supprt_Vector_Machine.html",
            "date": " • Oct 23, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Multiple Linear Regression",
            "content": "0.Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;50_Startups.csv&#39;) dataset . R&amp;D Spend Administration Marketing Spend State Profit . 0 165349.20 | 136897.80 | 471784.10 | New York | 192261.83 | . 1 162597.70 | 151377.59 | 443898.53 | California | 191792.06 | . 2 153441.51 | 101145.55 | 407934.54 | Florida | 191050.39 | . 3 144372.41 | 118671.85 | 383199.62 | New York | 182901.99 | . 4 142107.34 | 91391.77 | 366168.42 | Florida | 166187.94 | . 5 131876.90 | 99814.71 | 362861.36 | New York | 156991.12 | . 6 134615.46 | 147198.87 | 127716.82 | California | 156122.51 | . 7 130298.13 | 145530.06 | 323876.68 | Florida | 155752.60 | . 8 120542.52 | 148718.95 | 311613.29 | New York | 152211.77 | . 9 123334.88 | 108679.17 | 304981.62 | California | 149759.96 | . 10 101913.08 | 110594.11 | 229160.95 | Florida | 146121.95 | . 11 100671.96 | 91790.61 | 249744.55 | California | 144259.40 | . 12 93863.75 | 127320.38 | 249839.44 | Florida | 141585.52 | . 13 91992.39 | 135495.07 | 252664.93 | California | 134307.35 | . 14 119943.24 | 156547.42 | 256512.92 | Florida | 132602.65 | . 15 114523.61 | 122616.84 | 261776.23 | New York | 129917.04 | . 16 78013.11 | 121597.55 | 264346.06 | California | 126992.93 | . 17 94657.16 | 145077.58 | 282574.31 | New York | 125370.37 | . 18 91749.16 | 114175.79 | 294919.57 | Florida | 124266.90 | . 19 86419.70 | 153514.11 | 0.00 | New York | 122776.86 | . 20 76253.86 | 113867.30 | 298664.47 | California | 118474.03 | . 21 78389.47 | 153773.43 | 299737.29 | New York | 111313.02 | . 22 73994.56 | 122782.75 | 303319.26 | Florida | 110352.25 | . 23 67532.53 | 105751.03 | 304768.73 | Florida | 108733.99 | . 24 77044.01 | 99281.34 | 140574.81 | New York | 108552.04 | . 25 64664.71 | 139553.16 | 137962.62 | California | 107404.34 | . 26 75328.87 | 144135.98 | 134050.07 | Florida | 105733.54 | . 27 72107.60 | 127864.55 | 353183.81 | New York | 105008.31 | . 28 66051.52 | 182645.56 | 118148.20 | Florida | 103282.38 | . 29 65605.48 | 153032.06 | 107138.38 | New York | 101004.64 | . 30 61994.48 | 115641.28 | 91131.24 | Florida | 99937.59 | . 31 61136.38 | 152701.92 | 88218.23 | New York | 97483.56 | . 32 63408.86 | 129219.61 | 46085.25 | California | 97427.84 | . 33 55493.95 | 103057.49 | 214634.81 | Florida | 96778.92 | . 34 46426.07 | 157693.92 | 210797.67 | California | 96712.80 | . 35 46014.02 | 85047.44 | 205517.64 | New York | 96479.51 | . 36 28663.76 | 127056.21 | 201126.82 | Florida | 90708.19 | . 37 44069.95 | 51283.14 | 197029.42 | California | 89949.14 | . 38 20229.59 | 65947.93 | 185265.10 | New York | 81229.06 | . 39 38558.51 | 82982.09 | 174999.30 | California | 81005.76 | . 40 28754.33 | 118546.05 | 172795.67 | California | 78239.91 | . 41 27892.92 | 84710.77 | 164470.71 | Florida | 77798.83 | . 42 23640.93 | 96189.63 | 148001.11 | California | 71498.49 | . 43 15505.73 | 127382.30 | 35534.17 | New York | 69758.98 | . 44 22177.74 | 154806.14 | 28334.72 | California | 65200.33 | . 45 1000.23 | 124153.04 | 1903.93 | New York | 64926.08 | . 46 1315.46 | 115816.21 | 297114.46 | Florida | 49490.75 | . 47 0.00 | 135426.92 | 0.00 | California | 42559.73 | . 48 542.05 | 51743.15 | 0.00 | New York | 35673.41 | . 49 0.00 | 116983.80 | 45173.06 | California | 14681.40 | . 0.3 Check if any null value . dataset.isna().sum() . R&amp;D Spend 0 Administration 0 Marketing Spend 0 State 0 Profit 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 50 entries, 0 to 49 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 R&amp;D Spend 50 non-null float64 1 Administration 50 non-null float64 2 Marketing Spend 50 non-null float64 3 State 50 non-null object 4 Profit 50 non-null float64 dtypes: float64(4), object(1) memory usage: 2.1+ KB . 0.4 Split into X &amp; y . X = dataset.drop(&#39;Profit&#39;, axis=1) X . R&amp;D Spend Administration Marketing Spend State . 0 165349.20 | 136897.80 | 471784.10 | New York | . 1 162597.70 | 151377.59 | 443898.53 | California | . 2 153441.51 | 101145.55 | 407934.54 | Florida | . 3 144372.41 | 118671.85 | 383199.62 | New York | . 4 142107.34 | 91391.77 | 366168.42 | Florida | . 5 131876.90 | 99814.71 | 362861.36 | New York | . 6 134615.46 | 147198.87 | 127716.82 | California | . 7 130298.13 | 145530.06 | 323876.68 | Florida | . 8 120542.52 | 148718.95 | 311613.29 | New York | . 9 123334.88 | 108679.17 | 304981.62 | California | . 10 101913.08 | 110594.11 | 229160.95 | Florida | . 11 100671.96 | 91790.61 | 249744.55 | California | . 12 93863.75 | 127320.38 | 249839.44 | Florida | . 13 91992.39 | 135495.07 | 252664.93 | California | . 14 119943.24 | 156547.42 | 256512.92 | Florida | . 15 114523.61 | 122616.84 | 261776.23 | New York | . 16 78013.11 | 121597.55 | 264346.06 | California | . 17 94657.16 | 145077.58 | 282574.31 | New York | . 18 91749.16 | 114175.79 | 294919.57 | Florida | . 19 86419.70 | 153514.11 | 0.00 | New York | . 20 76253.86 | 113867.30 | 298664.47 | California | . 21 78389.47 | 153773.43 | 299737.29 | New York | . 22 73994.56 | 122782.75 | 303319.26 | Florida | . 23 67532.53 | 105751.03 | 304768.73 | Florida | . 24 77044.01 | 99281.34 | 140574.81 | New York | . 25 64664.71 | 139553.16 | 137962.62 | California | . 26 75328.87 | 144135.98 | 134050.07 | Florida | . 27 72107.60 | 127864.55 | 353183.81 | New York | . 28 66051.52 | 182645.56 | 118148.20 | Florida | . 29 65605.48 | 153032.06 | 107138.38 | New York | . 30 61994.48 | 115641.28 | 91131.24 | Florida | . 31 61136.38 | 152701.92 | 88218.23 | New York | . 32 63408.86 | 129219.61 | 46085.25 | California | . 33 55493.95 | 103057.49 | 214634.81 | Florida | . 34 46426.07 | 157693.92 | 210797.67 | California | . 35 46014.02 | 85047.44 | 205517.64 | New York | . 36 28663.76 | 127056.21 | 201126.82 | Florida | . 37 44069.95 | 51283.14 | 197029.42 | California | . 38 20229.59 | 65947.93 | 185265.10 | New York | . 39 38558.51 | 82982.09 | 174999.30 | California | . 40 28754.33 | 118546.05 | 172795.67 | California | . 41 27892.92 | 84710.77 | 164470.71 | Florida | . 42 23640.93 | 96189.63 | 148001.11 | California | . 43 15505.73 | 127382.30 | 35534.17 | New York | . 44 22177.74 | 154806.14 | 28334.72 | California | . 45 1000.23 | 124153.04 | 1903.93 | New York | . 46 1315.46 | 115816.21 | 297114.46 | Florida | . 47 0.00 | 135426.92 | 0.00 | California | . 48 542.05 | 51743.15 | 0.00 | New York | . 49 0.00 | 116983.80 | 45173.06 | California | . y = dataset[&#39;Profit&#39;] y . 0 192261.83 1 191792.06 2 191050.39 3 182901.99 4 166187.94 5 156991.12 6 156122.51 7 155752.60 8 152211.77 9 149759.96 10 146121.95 11 144259.40 12 141585.52 13 134307.35 14 132602.65 15 129917.04 16 126992.93 17 125370.37 18 124266.90 19 122776.86 20 118474.03 21 111313.02 22 110352.25 23 108733.99 24 108552.04 25 107404.34 26 105733.54 27 105008.31 28 103282.38 29 101004.64 30 99937.59 31 97483.56 32 97427.84 33 96778.92 34 96712.80 35 96479.51 36 90708.19 37 89949.14 38 81229.06 39 81005.76 40 78239.91 41 77798.83 42 71498.49 43 69758.98 44 65200.33 45 64926.08 46 49490.75 47 42559.73 48 35673.41 49 14681.40 Name: Profit, dtype: float64 . 0.5 Encoding categorical data . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_feature = [&quot;State&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_feature)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) . pd.DataFrame(transformed_X).head() . 0 1 2 3 4 5 . 0 0.0 | 0.0 | 1.0 | 165349.20 | 136897.80 | 471784.10 | . 1 1.0 | 0.0 | 0.0 | 162597.70 | 151377.59 | 443898.53 | . 2 0.0 | 1.0 | 0.0 | 153441.51 | 101145.55 | 407934.54 | . 3 0.0 | 0.0 | 1.0 | 144372.41 | 118671.85 | 383199.62 | . 4 0.0 | 1.0 | 0.0 | 142107.34 | 91391.77 | 366168.42 | . 0.6 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size = 0.25, random_state = 2509) . 1. Training the Multiple Linear Regression model on the Training set . from sklearn.linear_model import LinearRegression regressor = LinearRegression() regressor.fit(X_train, y_train) . LinearRegression() . 1.1 Score . regressor.score(X_test,y_test) . 0.9840064291741994 . 2. Predicting the Test set results . y_pred = regressor.predict(X_test) . d = {&#39;y_pred&#39;: y_pred, &#39;y_test&#39;: y_test} . 2.1 Compare Predicted results . pd.DataFrame(d) . y_pred y_test . 32 98884.371543 | 97427.84 | . 33 100047.235184 | 96778.92 | . 47 47766.247901 | 42559.73 | . 9 154976.558305 | 149759.96 | . 37 91129.087779 | 89949.14 | . 8 151755.926389 | 152211.77 | . 23 112436.195860 | 108733.99 | . 24 113375.898676 | 108552.04 | . 17 130706.106786 | 125370.37 | . 1 189141.730655 | 191792.06 | . 39 85217.422839 | 81005.76 | . 22 116952.737156 | 110352.25 | . 46 60343.602070 | 49490.75 | .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/regression/2020/10/22/multiple_linear_regression.html",
            "relUrl": "/jupyter/regression/2020/10/22/multiple_linear_regression.html",
            "date": " • Oct 22, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "K-Nearest Neighbors (K-NN)",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Social_Network_Ads.csv&#39;) dataset . User ID Gender Age EstimatedSalary Purchased . 0 15624510 | Male | 19 | 19000 | 0 | . 1 15810944 | Male | 35 | 20000 | 0 | . 2 15668575 | Female | 26 | 43000 | 0 | . 3 15603246 | Female | 27 | 57000 | 0 | . 4 15804002 | Male | 19 | 76000 | 0 | . ... ... | ... | ... | ... | ... | . 395 15691863 | Female | 46 | 41000 | 1 | . 396 15706071 | Male | 51 | 23000 | 1 | . 397 15654296 | Female | 50 | 20000 | 1 | . 398 15755018 | Male | 36 | 33000 | 0 | . 399 15594041 | Female | 49 | 36000 | 1 | . 400 rows × 5 columns . 0.3 Check if any null value . dataset.isna().sum() . User ID 0 Gender 0 Age 0 EstimatedSalary 0 Purchased 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 400 entries, 0 to 399 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 User ID 400 non-null int64 1 Gender 400 non-null object 2 Age 400 non-null int64 3 EstimatedSalary 400 non-null int64 4 Purchased 400 non-null int64 dtypes: int64(4), object(1) memory usage: 15.8+ KB . Drop User ID . dataset.drop(&#39;User ID&#39;, axis=1, inplace=True) dataset.head() . Gender Age EstimatedSalary Purchased . 0 Male | 19 | 19000 | 0 | . 1 Male | 35 | 20000 | 0 | . 2 Female | 26 | 43000 | 0 | . 3 Female | 27 | 57000 | 0 | . 4 Male | 19 | 76000 | 0 | . 0.4 Split into X &amp; y . X = dataset.drop(&#39;Purchased&#39;, axis=1) X.head() . Gender Age EstimatedSalary . 0 Male | 19 | 19000 | . 1 Male | 35 | 20000 | . 2 Female | 26 | 43000 | . 3 Female | 27 | 57000 | . 4 Male | 19 | 76000 | . y = dataset[&#39;Purchased&#39;] y.head() . 0 0 1 0 2 0 3 0 4 0 Name: Purchased, dtype: int64 . 0.5 Convert categories into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_feature = [&quot;Gender&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_feature)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) . pd.DataFrame(transformed_X).head() . 0 1 2 3 . 0 0.0 | 1.0 | 19.0 | 19000.0 | . 1 0.0 | 1.0 | 35.0 | 20000.0 | . 2 1.0 | 0.0 | 26.0 | 43000.0 | . 3 1.0 | 0.0 | 27.0 | 57000.0 | . 4 0.0 | 1.0 | 19.0 | 76000.0 | . 0.6 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size = 0.25, random_state = 2509) . 0.7 Feature Scaling . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) . 1.Training the model on the Training set . from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier(n_neighbors = 5, metric = &#39;minkowski&#39;, p = 2) classifier.fit(X_train, y_train) . KNeighborsClassifier() . 1.1 Score . classifier.score(X_test,y_test) . 0.93 . 2.Predicting the Test set results . y_pred = classifier.predict(X_test) . 2.2 Making the Confusion Matrix . from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_test, y_pred) print(cm) . [[66 4] [ 3 27]] .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/classification/2020/10/22/k_nearest_neighbors.html",
            "relUrl": "/jupyter/classification/2020/10/22/k_nearest_neighbors.html",
            "date": " • Oct 22, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Simple Linear Regression",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Salary_Data.csv&#39;) dataset . YearsExperience Salary . 0 1.1 | 39343.0 | . 1 1.3 | 46205.0 | . 2 1.5 | 37731.0 | . 3 2.0 | 43525.0 | . 4 2.2 | 39891.0 | . 5 2.9 | 56642.0 | . 6 3.0 | 60150.0 | . 7 3.2 | 54445.0 | . 8 3.2 | 64445.0 | . 9 3.7 | 57189.0 | . 10 3.9 | 63218.0 | . 11 4.0 | 55794.0 | . 12 4.0 | 56957.0 | . 13 4.1 | 57081.0 | . 14 4.5 | 61111.0 | . 15 4.9 | 67938.0 | . 16 5.1 | 66029.0 | . 17 5.3 | 83088.0 | . 18 5.9 | 81363.0 | . 19 6.0 | 93940.0 | . 20 6.8 | 91738.0 | . 21 7.1 | 98273.0 | . 22 7.9 | 101302.0 | . 23 8.2 | 113812.0 | . 24 8.7 | 109431.0 | . 25 9.0 | 105582.0 | . 26 9.5 | 116969.0 | . 27 9.6 | 112635.0 | . 28 10.3 | 122391.0 | . 29 10.5 | 121872.0 | . 0.3 Check if any null value . dataset.isna().sum() . YearsExperience 0 Salary 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 30 entries, 0 to 29 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 YearsExperience 30 non-null float64 1 Salary 30 non-null float64 dtypes: float64(2) memory usage: 608.0 bytes . 0.4 Split into X &amp; y . X = dataset.drop(&#39;Salary&#39;, axis=1) X . YearsExperience . 0 1.1 | . 1 1.3 | . 2 1.5 | . 3 2.0 | . 4 2.2 | . 5 2.9 | . 6 3.0 | . 7 3.2 | . 8 3.2 | . 9 3.7 | . 10 3.9 | . 11 4.0 | . 12 4.0 | . 13 4.1 | . 14 4.5 | . 15 4.9 | . 16 5.1 | . 17 5.3 | . 18 5.9 | . 19 6.0 | . 20 6.8 | . 21 7.1 | . 22 7.9 | . 23 8.2 | . 24 8.7 | . 25 9.0 | . 26 9.5 | . 27 9.6 | . 28 10.3 | . 29 10.5 | . y = dataset[&#39;Salary&#39;] y . 0 39343.0 1 46205.0 2 37731.0 3 43525.0 4 39891.0 5 56642.0 6 60150.0 7 54445.0 8 64445.0 9 57189.0 10 63218.0 11 55794.0 12 56957.0 13 57081.0 14 61111.0 15 67938.0 16 66029.0 17 83088.0 18 81363.0 19 93940.0 20 91738.0 21 98273.0 22 101302.0 23 113812.0 24 109431.0 25 105582.0 26 116969.0 27 112635.0 28 122391.0 29 121872.0 Name: Salary, dtype: float64 . 0.5 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0) . 1. Training the Simple Linear Regression model on the Training set . from sklearn.linear_model import LinearRegression regressor = LinearRegression() regressor.fit(X_train, y_train) . LinearRegression() . 1.1 Score . regressor.score(X_test,y_test) . 0.9740993407213511 . 2. Predicting the Test set results . y_pred = regressor.predict(X_test) . d = {&#39;y_pred&#39;: y_pred, &#39;y_test&#39;: y_test} . 2.1 Compare predicted results . pd.DataFrame(d) . y_pred y_test . 2 40817.783270 | 37731.0 | . 28 123188.082589 | 122391.0 | . 13 65154.462615 | 57081.0 | . 10 63282.410357 | 63218.0 | . 26 115699.873560 | 116969.0 | . 24 108211.664531 | 109431.0 | . 27 116635.899689 | 112635.0 | . 11 64218.436486 | 55794.0 | . 17 76386.776158 | 83088.0 | . 3. Visualising . 3.1 Visualising the Training set results . plt.scatter(X_train, y_train, color = &#39;red&#39;) plt.plot(X_train, regressor.predict(X_train), color = &#39;blue&#39;) plt.title(&#39;Salary vs Experience (Training set)&#39;) plt.xlabel(&#39;Years of Experience&#39;) plt.ylabel(&#39;Salary&#39;) plt.show() . 3.1 Visualising the Test set results . plt.scatter(X_test, y_test, color = &#39;red&#39;) plt.plot(X_train, regressor.predict(X_train), color = &#39;blue&#39;) plt.title(&#39;Salary vs Experience (Test set)&#39;) plt.xlabel(&#39;Years of Experience&#39;) plt.ylabel(&#39;Salary&#39;) plt.show() .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/regression/2020/10/21/simple_linear_regression.html",
            "relUrl": "/jupyter/regression/2020/10/21/simple_linear_regression.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Logistic Regression",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Social_Network_Ads.csv&#39;) dataset . User ID Gender Age EstimatedSalary Purchased . 0 15624510 | Male | 19 | 19000 | 0 | . 1 15810944 | Male | 35 | 20000 | 0 | . 2 15668575 | Female | 26 | 43000 | 0 | . 3 15603246 | Female | 27 | 57000 | 0 | . 4 15804002 | Male | 19 | 76000 | 0 | . ... ... | ... | ... | ... | ... | . 395 15691863 | Female | 46 | 41000 | 1 | . 396 15706071 | Male | 51 | 23000 | 1 | . 397 15654296 | Female | 50 | 20000 | 1 | . 398 15755018 | Male | 36 | 33000 | 0 | . 399 15594041 | Female | 49 | 36000 | 1 | . 400 rows × 5 columns . 0.3 Check if any null value . dataset.isna().sum() . User ID 0 Gender 0 Age 0 EstimatedSalary 0 Purchased 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 400 entries, 0 to 399 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 User ID 400 non-null int64 1 Gender 400 non-null object 2 Age 400 non-null int64 3 EstimatedSalary 400 non-null int64 4 Purchased 400 non-null int64 dtypes: int64(4), object(1) memory usage: 15.8+ KB . dataset.drop(&#39;User ID&#39;, axis=1, inplace=True) . dataset.head() . Gender Age EstimatedSalary Purchased . 0 Male | 19 | 19000 | 0 | . 1 Male | 35 | 20000 | 0 | . 2 Female | 26 | 43000 | 0 | . 3 Female | 27 | 57000 | 0 | . 4 Male | 19 | 76000 | 0 | . 0.4 Split into X &amp; y . X = dataset.drop(&#39;Purchased&#39;, axis=1) X.head() . Gender Age EstimatedSalary . 0 Male | 19 | 19000 | . 1 Male | 35 | 20000 | . 2 Female | 26 | 43000 | . 3 Female | 27 | 57000 | . 4 Male | 19 | 76000 | . y = dataset[&#39;Purchased&#39;] y.head() . 0 0 1 0 2 0 3 0 4 0 Name: Purchased, dtype: int64 . 0.5 Convert categories into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_feature = [&quot;Gender&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_feature)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) . pd.DataFrame(transformed_X).head() . 0 1 2 3 . 0 0.0 | 1.0 | 19.0 | 19000.0 | . 1 0.0 | 1.0 | 35.0 | 20000.0 | . 2 1.0 | 0.0 | 26.0 | 43000.0 | . 3 1.0 | 0.0 | 27.0 | 57000.0 | . 4 0.0 | 1.0 | 19.0 | 76000.0 | . 0.6 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size = 0.25, random_state = 2509) . 0.7 Feature Scaling . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) . 1. Training the model on the Training set . from sklearn.linear_model import LogisticRegression classifier = LogisticRegression() classifier.fit(X_train, y_train) . LogisticRegression() . 1.1 Score . LR_score = classifier.score(X_test, y_test) LR_score . 0.89 . 2. Predicting the Test set results . y_pred = classifier.predict(X_test) . 2.1 Making the Confusion Matrix . from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_test, y_pred) print(cm) . [[65 5] [ 6 24]] .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/classification/2020/10/21/logistic_regression.html",
            "relUrl": "/jupyter/classification/2020/10/21/logistic_regression.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Data Preprocessing",
            "content": "1.Import library . import numpy as np import pandas as pd import matplotlib.pyplot as plt . 2.Get the data . car_sales = pd.read_csv(&quot;car-sales-data.csv&quot;) car_sales.head() . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . 3.Check for missing values . car_sales.isna().sum() . Make 49 Colour 50 Odometer (KM) 50 Doors 50 Price 50 dtype: int64 . 3.1 What if data is filled with missing values? . Drop the rows with no labels | Fill them with some value(also known as imputation). | 1.Drop the rows with no labels . car_sales.dropna(subset=[&quot;Price&quot;],inplace=True) car_sales.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . car_sales . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . ... ... | ... | ... | ... | ... | . 995 Toyota | Black | 35820.0 | 4.0 | 32042.0 | . 996 NaN | White | 155144.0 | 3.0 | 5716.0 | . 997 Nissan | Blue | 66604.0 | 4.0 | 31570.0 | . 998 Honda | White | 215883.0 | 4.0 | 4001.0 | . 999 Toyota | Blue | 248360.0 | 4.0 | 12732.0 | . 950 rows × 5 columns . 2.Fill them with some value . Option 1. Fill missing data with pandas | Option 2. Fill missing data with scikit learn | . 2.1 Option 1. With Pandas . car_sales.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . car_sales.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 950 entries, 0 to 999 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 Make 903 non-null object 1 Colour 904 non-null object 2 Odometer (KM) 902 non-null float64 3 Doors 903 non-null float64 4 Price 950 non-null float64 dtypes: float64(3), object(2) memory usage: 44.5+ KB . car_sales[&quot;Make&quot;].fillna(&quot;missing&quot;,inplace=True) car_sales[&quot;Colour&quot;].fillna(&quot;missing&quot;,inplace=True) car_sales[&quot;Odometer (KM)&quot;].fillna(car_sales[&quot;Odometer (KM)&quot;].median(),inplace=True) car_sales[&quot;Doors&quot;].fillna(4,inplace=True) . car_sales . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . ... ... | ... | ... | ... | ... | . 995 Toyota | Black | 35820.0 | 4.0 | 32042.0 | . 996 missing | White | 155144.0 | 3.0 | 5716.0 | . 997 Nissan | Blue | 66604.0 | 4.0 | 31570.0 | . 998 Honda | White | 215883.0 | 4.0 | 4001.0 | . 999 Toyota | Blue | 248360.0 | 4.0 | 12732.0 | . 950 rows × 5 columns . car_sales.isna().sum() . Make 0 Colour 0 Odometer (KM) 0 Doors 0 Price 0 dtype: int64 . 2.2 Option 2. With scikit learn . car_sales_missing = pd.read_csv(&quot;car-sales-data.csv&quot;) car_sales_missing . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . ... ... | ... | ... | ... | ... | . 995 Toyota | Black | 35820.0 | 4.0 | 32042.0 | . 996 NaN | White | 155144.0 | 3.0 | 5716.0 | . 997 Nissan | Blue | 66604.0 | 4.0 | 31570.0 | . 998 Honda | White | 215883.0 | 4.0 | 4001.0 | . 999 Toyota | Blue | 248360.0 | 4.0 | 12732.0 | . 1000 rows × 5 columns . car_sales_missing.dropna(subset=[&quot;Price&quot;],inplace=True) car_sales_missing.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . car_sales_missing.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 950 entries, 0 to 999 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 Make 903 non-null object 1 Colour 904 non-null object 2 Odometer (KM) 902 non-null float64 3 Doors 903 non-null float64 4 Price 950 non-null float64 dtypes: float64(3), object(2) memory usage: 44.5+ KB . If you are use scikit learn to fill missing value, then you have to Split data into X and y . X = car_sales_missing.drop(&quot;Price&quot;,axis=1) y = car_sales_missing[&quot;Price&quot;] . from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer # Fill categorical values with &#39;missing&#39; &amp; numerical values with median cat_imputer = SimpleImputer(strategy=&quot;constant&quot;, fill_value=&quot;missing&quot;) door_imputer = SimpleImputer(strategy=&quot;constant&quot;, fill_value=4) num_imputer = SimpleImputer(strategy=&quot;median&quot;) # Define columns cat_features = [&quot;Make&quot;, &quot;Colour&quot;] door_feature = [&quot;Doors&quot;] num_features = [&quot;Odometer (KM)&quot;] # Create an imputer (something that fills missing data) imputer = ColumnTransformer([ (&quot;cat_imputer&quot;, cat_imputer, cat_features), (&quot;door_imputer&quot;, door_imputer, door_feature), (&quot;num_imputer&quot;, num_imputer, num_features) ]) filled_X = imputer.fit_transform(X) filled_X . array([[&#39;Honda&#39;, &#39;White&#39;, 4.0, 35431.0], [&#39;BMW&#39;, &#39;Blue&#39;, 5.0, 192714.0], [&#39;Honda&#39;, &#39;White&#39;, 4.0, 84714.0], ..., [&#39;Nissan&#39;, &#39;Blue&#39;, 4.0, 66604.0], [&#39;Honda&#39;, &#39;White&#39;, 4.0, 215883.0], [&#39;Toyota&#39;, &#39;Blue&#39;, 4.0, 248360.0]], dtype=object) . car_sales_filled = pd.DataFrame(filled_X, columns=[&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;, &quot;Odometer (KM)&quot;]) car_sales_filled.isna().sum() . Make 0 Colour 0 Doors 0 Odometer (KM) 0 dtype: int64 . car_sales_filled . Make Colour Doors Odometer (KM) . 0 Honda | White | 4 | 35431 | . 1 BMW | Blue | 5 | 192714 | . 2 Honda | White | 4 | 84714 | . 3 Toyota | White | 4 | 154365 | . 4 Nissan | Blue | 3 | 181577 | . ... ... | ... | ... | ... | . 945 Toyota | Black | 4 | 35820 | . 946 missing | White | 3 | 155144 | . 947 Nissan | Blue | 4 | 66604 | . 948 Honda | White | 4 | 215883 | . 949 Toyota | Blue | 4 | 248360 | . 950 rows × 4 columns . Convert categorical data into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_features = [&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_features)], remainder=&quot;passthrough&quot;) # Fill train and test values separately transformed_X = transformer.fit_transform(car_sales_filled) # Check transformed and filled X_train transformed_X.toarray() . array([[0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 3.54310e+04], [1.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00, 1.00000e+00, 1.92714e+05], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 8.47140e+04], ..., [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 1.00000e+00, 0.00000e+00, 6.66040e+04], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 2.15883e+05], [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 2.48360e+05]]) . Split data into train and test . from sklearn.model_selection import train_test_split np.random.seed(2509) X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.2) . X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((760, 15), (190, 15), (760,), (190,)) . Now data is in write shape to fit into model . Happy coding and have a great time learning how to make machines smarter. .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/data%20preprocessing/2020/10/20/Data_preprocessing.html",
            "relUrl": "/jupyter/data%20preprocessing/2020/10/20/Data_preprocessing.html",
            "date": " • Oct 20, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Basic machine learning path",
            "content": "Basic path to learn Practical Machine Learning. . Prerequisite . High school math(vectors, matrices, calculus, probability, and stats) | Basic Python Help. | Must have Patience to learn new things. | . . Motivation . Watch AI For Everyone By Andrew Ng . The meaning behind common AI terminology, including neural networks, machine learning, deep learning, and data science. | What AI realistically can–and cannot–do. | How to spot opportunities to apply AI to problems in your own organization. | What it feels like to build machine learning and data science projects. | How to work with an AI team and build an AI strategy in your company. | How to navigate ethical and societal discussions surrounding AI. | . | YouTube Originals AGE OF AI . How AI is used in real life. . | . Started learning . Step-0 . Understand basic of machine learning Supervised Learning | Unsupervised Learning | Classification and Regression | . | Learn python and some useful library Pandas Pandas is a popular Python library for data analysis. . | NumPy NumPy is a very popular python library for large multi-dimensional array and matrix processing. . | Matplotlib Matpoltlib is a very popular Python library for data visualization. . | . | Setup Local Machine with latest Anaconda Anaconda is a free and open-source distribution of the Python and R. . | Step-1 . How to use data from verious source like [Kaggle UCI ml repo]. . | Start to use Jupyter notebook A complate IDE for data science and machine learning. . | Started hand on practice with scikit-learn. . | Use scikit-learn map and documentation. . | Step-2 . Now, you are a little bit comfortable with coding it’s time to learn basic maths behind those algorithms. . | Take a Andrew’s Course. . | . Step-3 . Learn about Deep-learning. . | Learn about popular library [TensorFlow PyTorch] TensorFlow is backed by Google Brain team. | PyTorch is developed by Facebook’s AI Research lab. | Both have large community. | There are other librarys as well like Theano, Keras, Caffe, Apache MXNet and many more. | . | In neural network learn ANN (artificial neural network) | CNN (Convolutional neural network) | RNN (Recurrent neural networks) | Autoencoder | | . . Happy coding and have a great time learning how to make machines smarter. .",
            "url": "https://vivek2509.github.io/World_of_ML/markdown/2020/10/19/Basic-ML-path.html",
            "relUrl": "/markdown/2020/10/19/Basic-ML-path.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Start writing blog",
            "content": "Start writing blog within 10 min . How to start write blog direct from Github using Markdown and Jupyter notebook. . . Requirement . github account. | . Follow step . Use fastai/fastpages template. Within 20-30 sec one pull request is generated by github-action. | Follow step and create SSH key from this and add public and private key. | Merge pull request and wait for 2-3 min for github-action to build the site. | Change Name and description . open _config.yml in edit mode. | change name and description | you can add social links as well | . . . . Adding stuff . Make sure are you add stuff into direct master OR main branch. | You can add notebook, markdown and words file into _notebook , _posts and _word folder accordingly with right name formet. | You can upload all your local images in images folder | . . . . Happy blogging .",
            "url": "https://vivek2509.github.io/World_of_ML/markdown/2020/10/18/How-to-start-blog-from-fastpage.html",
            "relUrl": "/markdown/2020/10/18/How-to-start-blog-from-fastpage.html",
            "date": " • Oct 18, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". AI/ML enthusiast with ♥ in photography 📸. .",
          "url": "https://vivek2509.github.io/World_of_ML/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://vivek2509.github.io/World_of_ML/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}