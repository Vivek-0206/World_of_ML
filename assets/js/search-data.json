{
  
    
        "post0": {
            "title": "Logistic Regression",
            "content": "0. Data Preprocessing . Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . Importing the dataset . dataset = pd.read_csv(&#39;Social_Network_Ads.csv&#39;) dataset . User ID Gender Age EstimatedSalary Purchased . 0 15624510 | Male | 19 | 19000 | 0 | . 1 15810944 | Male | 35 | 20000 | 0 | . 2 15668575 | Female | 26 | 43000 | 0 | . 3 15603246 | Female | 27 | 57000 | 0 | . 4 15804002 | Male | 19 | 76000 | 0 | . ... ... | ... | ... | ... | ... | . 395 15691863 | Female | 46 | 41000 | 1 | . 396 15706071 | Male | 51 | 23000 | 1 | . 397 15654296 | Female | 50 | 20000 | 1 | . 398 15755018 | Male | 36 | 33000 | 0 | . 399 15594041 | Female | 49 | 36000 | 1 | . 400 rows × 5 columns . Check if any null value . dataset.isna().sum() . User ID 0 Gender 0 Age 0 EstimatedSalary 0 Purchased 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 400 entries, 0 to 399 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 User ID 400 non-null int64 1 Gender 400 non-null object 2 Age 400 non-null int64 3 EstimatedSalary 400 non-null int64 4 Purchased 400 non-null int64 dtypes: int64(4), object(1) memory usage: 15.8+ KB . dataset.drop(&#39;User ID&#39;, axis=1, inplace=True) . dataset.head() . Gender Age EstimatedSalary Purchased . 0 Male | 19 | 19000 | 0 | . 1 Male | 35 | 20000 | 0 | . 2 Female | 26 | 43000 | 0 | . 3 Female | 27 | 57000 | 0 | . 4 Male | 19 | 76000 | 0 | . Split into X &amp; y . X = dataset.drop(&#39;Purchased&#39;, axis=1) X.head() . Gender Age EstimatedSalary . 0 Male | 19 | 19000 | . 1 Male | 35 | 20000 | . 2 Female | 26 | 43000 | . 3 Female | 27 | 57000 | . 4 Male | 19 | 76000 | . y = dataset[&#39;Purchased&#39;] y.head() . 0 0 1 0 2 0 3 0 4 0 Name: Purchased, dtype: int64 . Convert categories into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_feature = [&quot;Gender&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_feature)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) . pd.DataFrame(transformed_X).head() . 0 1 2 3 . 0 0.0 | 1.0 | 19.0 | 19000.0 | . 1 0.0 | 1.0 | 35.0 | 20000.0 | . 2 1.0 | 0.0 | 26.0 | 43000.0 | . 3 1.0 | 0.0 | 27.0 | 57000.0 | . 4 0.0 | 1.0 | 19.0 | 76000.0 | . Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size = 0.25, random_state = 2509) . Feature Scaling . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) . 1.Training the model on the Training set . from sklearn.linear_model import LogisticRegression classifier = LogisticRegression() classifier.fit(X_train, y_train) . LogisticRegression() . LR_score = classifier.score(X_test, y_test) LR_score . 0.89 . 2.Predicting the Test set results . y_pred = classifier.predict(X_test) . Making the Confusion Matrix . from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_test, y_pred) print(cm) . [[65 5] [ 6 24]] .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/classification/2020/10/21/logistic_regression.html",
            "relUrl": "/jupyter/classification/2020/10/21/logistic_regression.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Data Preprocessing",
            "content": "1.Import library . import numpy as np import pandas as pd import matplotlib.pyplot as plt . 2.Get the data . car_sales = pd.read_csv(&quot;car-sales-data.csv&quot;) car_sales.head() . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . 3.Check for missing values . car_sales.isna().sum() . Make 49 Colour 50 Odometer (KM) 50 Doors 50 Price 50 dtype: int64 . 3.1 What if data is filled with missing values? . Drop the rows with no labels | Fill them with some value(also known as imputation). | 1.Drop the rows with no labels . car_sales.dropna(subset=[&quot;Price&quot;],inplace=True) car_sales.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . car_sales . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . ... ... | ... | ... | ... | ... | . 995 Toyota | Black | 35820.0 | 4.0 | 32042.0 | . 996 NaN | White | 155144.0 | 3.0 | 5716.0 | . 997 Nissan | Blue | 66604.0 | 4.0 | 31570.0 | . 998 Honda | White | 215883.0 | 4.0 | 4001.0 | . 999 Toyota | Blue | 248360.0 | 4.0 | 12732.0 | . 950 rows × 5 columns . 2.Fill them with some value . Option 1. Fill missing data with pandas | Option 2. Fill missing data with scikit learn | . 2.1 Option 1. With Pandas . car_sales.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . car_sales.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 950 entries, 0 to 999 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 Make 903 non-null object 1 Colour 904 non-null object 2 Odometer (KM) 902 non-null float64 3 Doors 903 non-null float64 4 Price 950 non-null float64 dtypes: float64(3), object(2) memory usage: 44.5+ KB . car_sales[&quot;Make&quot;].fillna(&quot;missing&quot;,inplace=True) car_sales[&quot;Colour&quot;].fillna(&quot;missing&quot;,inplace=True) car_sales[&quot;Odometer (KM)&quot;].fillna(car_sales[&quot;Odometer (KM)&quot;].median(),inplace=True) car_sales[&quot;Doors&quot;].fillna(4,inplace=True) . car_sales . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . ... ... | ... | ... | ... | ... | . 995 Toyota | Black | 35820.0 | 4.0 | 32042.0 | . 996 missing | White | 155144.0 | 3.0 | 5716.0 | . 997 Nissan | Blue | 66604.0 | 4.0 | 31570.0 | . 998 Honda | White | 215883.0 | 4.0 | 4001.0 | . 999 Toyota | Blue | 248360.0 | 4.0 | 12732.0 | . 950 rows × 5 columns . car_sales.isna().sum() . Make 0 Colour 0 Odometer (KM) 0 Doors 0 Price 0 dtype: int64 . 2.2 Option 2. With scikit learn . car_sales_missing = pd.read_csv(&quot;car-sales-data.csv&quot;) car_sales_missing . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . ... ... | ... | ... | ... | ... | . 995 Toyota | Black | 35820.0 | 4.0 | 32042.0 | . 996 NaN | White | 155144.0 | 3.0 | 5716.0 | . 997 Nissan | Blue | 66604.0 | 4.0 | 31570.0 | . 998 Honda | White | 215883.0 | 4.0 | 4001.0 | . 999 Toyota | Blue | 248360.0 | 4.0 | 12732.0 | . 1000 rows × 5 columns . car_sales_missing.dropna(subset=[&quot;Price&quot;],inplace=True) car_sales_missing.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . car_sales_missing.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 950 entries, 0 to 999 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 Make 903 non-null object 1 Colour 904 non-null object 2 Odometer (KM) 902 non-null float64 3 Doors 903 non-null float64 4 Price 950 non-null float64 dtypes: float64(3), object(2) memory usage: 44.5+ KB . If you are use scikit learn to fill missing value, then you have to Split data into X and y . X = car_sales_missing.drop(&quot;Price&quot;,axis=1) y = car_sales_missing[&quot;Price&quot;] . from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer # Fill categorical values with &#39;missing&#39; &amp; numerical values with median cat_imputer = SimpleImputer(strategy=&quot;constant&quot;, fill_value=&quot;missing&quot;) door_imputer = SimpleImputer(strategy=&quot;constant&quot;, fill_value=4) num_imputer = SimpleImputer(strategy=&quot;median&quot;) # Define columns cat_features = [&quot;Make&quot;, &quot;Colour&quot;] door_feature = [&quot;Doors&quot;] num_features = [&quot;Odometer (KM)&quot;] # Create an imputer (something that fills missing data) imputer = ColumnTransformer([ (&quot;cat_imputer&quot;, cat_imputer, cat_features), (&quot;door_imputer&quot;, door_imputer, door_feature), (&quot;num_imputer&quot;, num_imputer, num_features) ]) filled_X = imputer.fit_transform(X) filled_X . array([[&#39;Honda&#39;, &#39;White&#39;, 4.0, 35431.0], [&#39;BMW&#39;, &#39;Blue&#39;, 5.0, 192714.0], [&#39;Honda&#39;, &#39;White&#39;, 4.0, 84714.0], ..., [&#39;Nissan&#39;, &#39;Blue&#39;, 4.0, 66604.0], [&#39;Honda&#39;, &#39;White&#39;, 4.0, 215883.0], [&#39;Toyota&#39;, &#39;Blue&#39;, 4.0, 248360.0]], dtype=object) . car_sales_filled = pd.DataFrame(filled_X, columns=[&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;, &quot;Odometer (KM)&quot;]) car_sales_filled.isna().sum() . Make 0 Colour 0 Doors 0 Odometer (KM) 0 dtype: int64 . car_sales_filled . Make Colour Doors Odometer (KM) . 0 Honda | White | 4 | 35431 | . 1 BMW | Blue | 5 | 192714 | . 2 Honda | White | 4 | 84714 | . 3 Toyota | White | 4 | 154365 | . 4 Nissan | Blue | 3 | 181577 | . ... ... | ... | ... | ... | . 945 Toyota | Black | 4 | 35820 | . 946 missing | White | 3 | 155144 | . 947 Nissan | Blue | 4 | 66604 | . 948 Honda | White | 4 | 215883 | . 949 Toyota | Blue | 4 | 248360 | . 950 rows × 4 columns . Convert categorical data into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_features = [&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_features)], remainder=&quot;passthrough&quot;) # Fill train and test values separately transformed_X = transformer.fit_transform(car_sales_filled) # Check transformed and filled X_train transformed_X.toarray() . array([[0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 3.54310e+04], [1.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00, 1.00000e+00, 1.92714e+05], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 8.47140e+04], ..., [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 1.00000e+00, 0.00000e+00, 6.66040e+04], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 2.15883e+05], [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 2.48360e+05]]) . Split data into train and test . from sklearn.model_selection import train_test_split np.random.seed(2509) X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.2) . X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((760, 15), (190, 15), (760,), (190,)) . Now data is in write shape to fit into model . Happy coding and have a great time learning how to make machines smarter. .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/data%20preprocessing/2020/10/20/Data_preprocessing.html",
            "relUrl": "/jupyter/data%20preprocessing/2020/10/20/Data_preprocessing.html",
            "date": " • Oct 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Basic machine learning path",
            "content": "Basic path to learn Practical Machine Learning. . Prerequisite . High school math(vectors, matrices, calculus, probability, and stats) | Basic Python Help. | Must have Patience to learn new things. | . . Motivation . Watch AI For Everyone By Andrew Ng . The meaning behind common AI terminology, including neural networks, machine learning, deep learning, and data science. | What AI realistically can–and cannot–do. | How to spot opportunities to apply AI to problems in your own organization. | What it feels like to build machine learning and data science projects. | How to work with an AI team and build an AI strategy in your company. | How to navigate ethical and societal discussions surrounding AI. | . | YouTube Originals AGE OF AI . How AI is used in real life. . | . Started learning . Step-0 . Understand basic of machine learning Supervised Learning | Unsupervised Learning | Classification and Regression | . | Learn python and some useful library Pandas Pandas is a popular Python library for data analysis. . | NumPy NumPy is a very popular python library for large multi-dimensional array and matrix processing. . | Matplotlib Matpoltlib is a very popular Python library for data visualization. . | . | Setup Local Machine with latest Anaconda Anaconda is a free and open-source distribution of the Python and R. . | Step-1 . How to use data from verious source like [Kaggle UCI ml repo]. . | Start to use Jupyter notebook A complate IDE for data science and machine learning. . | Started hand on practice with scikit-learn. . | Use scikit-learn map and documentation. . | Step-2 . Now, you are a little bit comfortable with coding it’s time to learn basic maths behind those algorithms. . | Take a Andrew’s Course. . | . Step-3 . Learn about Deep-learning. . | Learn about popular library [TensorFlow PyTorch] TensorFlow is backed by Google Brain team. | PyTorch is developed by Facebook’s AI Research lab. | Both have large community. | There are other librarys as well like Theano, Keras, Caffe, Apache MXNet and many more. | . | In neural network learn ANN (artificial neural network) | CNN (Convolutional neural network) | RNN (Recurrent neural networks) | Autoencoder | | . . Happy coding and have a great time learning how to make machines smarter. .",
            "url": "https://vivek2509.github.io/World_of_ML/markdown/2020/10/19/Basic-ML-path.html",
            "relUrl": "/markdown/2020/10/19/Basic-ML-path.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Start writing blog",
            "content": "Start writing blog within 10 min . How to start write blog direct from Github using Markdown and Jupyter notebook. . . Requirement . github account. | . Follow step . Use fastai/fastpages template. Within 20-30 sec one pull request is generated by github-action. | Follow step and create SSH key from this and add public and private key. | Merge pull request and wait for 2-3 min for github-action to build the site. | Change Name and description . open _config.yml in edit mode. | change name and description | you can add social links as well | . . . . Adding stuff . Make sure are you add stuff into direct master OR main branch. | You can add notebook, markdown and words file into _notebook , _posts and _word folder accordingly with right name formet. | You can upload all your local images in images folder | . . . . Happy blogging .",
            "url": "https://vivek2509.github.io/World_of_ML/markdown/2020/10/18/How-to-start-blog-from-fastpage.html",
            "relUrl": "/markdown/2020/10/18/How-to-start-blog-from-fastpage.html",
            "date": " • Oct 18, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". AI/ML enthusiast with ♥ in photography 📸. .",
          "url": "https://vivek2509.github.io/World_of_ML/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://vivek2509.github.io/World_of_ML/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}