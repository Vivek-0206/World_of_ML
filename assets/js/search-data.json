{
  
    
        "post0": {
            "title": "Naive bayes",
            "content": "0. Data preprocessing . 0.1 Import library . import pandas as pd import numpy as np import matplotlib.pyplot as plt . 0.2 Import dataset . email = pd.read_csv(&#39;emails.csv&#39;) email . text spam . 0 Subject: naturally irresistible your corporate... | 1 | . 1 Subject: the stock trading gunslinger fanny i... | 1 | . 2 Subject: unbelievable new homes made easy im ... | 1 | . 3 Subject: 4 color printing special request add... | 1 | . 4 Subject: do not have money , get software cds ... | 1 | . ... ... | ... | . 5723 Subject: re : research and development charges... | 0 | . 5724 Subject: re : receipts from visit jim , than... | 0 | . 5725 Subject: re : enron case study update wow ! a... | 0 | . 5726 Subject: re : interest david , please , call... | 0 | . 5727 Subject: news : aurora 5 . 2 update aurora ve... | 0 | . 5728 rows × 2 columns . email.head() . text spam . 0 Subject: naturally irresistible your corporate... | 1 | . 1 Subject: the stock trading gunslinger fanny i... | 1 | . 2 Subject: unbelievable new homes made easy im ... | 1 | . 3 Subject: 4 color printing special request add... | 1 | . 4 Subject: do not have money , get software cds ... | 1 | . len(email) . 5728 . 0.3 Check if any null value . email.isna().sum() . text 0 spam 0 dtype: int64 . email.shape . (5728, 2) . email.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5728 entries, 0 to 5727 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 text 5728 non-null object 1 spam 5728 non-null int64 dtypes: int64(1), object(1) memory usage: 89.6+ KB . 0.4 Cleaning the text . import re import nltk nltk.download(&#39;stopwords&#39;) from nltk.corpus import stopwords from nltk.stem.porter import PorterStemmer corpus = [] for i in range(0, len(email)): e_mail = re.sub(&#39;[^a-zA-Z]&#39;, &#39; &#39;, email[&#39;text&#39;][i]) e_mail = e_mail.split() ps = PorterStemmer() e_mail = [ps.stem(word) for word in e_mail if not word in set(stopwords.words(&#39;english&#39;))] e_mail = &#39; &#39;.join(e_mail) corpus.append(e_mail) . [nltk_data] Downloading package stopwords to [nltk_data] C: Users patel AppData Roaming nltk_data... [nltk_data] Package stopwords is already up-to-date! . corpus[2509] . &#39;subject enron mid year perform manag process enron mid year perform manag process begun process requir select suggest review provid perform relat feedback may also request provid feedback fellow employe need access perform manag system pep http pep enron com question direct pep help desk follow number u option europ option canada canada employe e mail question perfmgmt enron com log pep enter user id password provid log immedi prompt chang secur password user id password user id wkamin password welcom&#39; . 0.5 Creating the Bag of Words model . from sklearn.feature_extraction.text import CountVectorizer cv = CountVectorizer() X = cv.fit_transform(corpus).toarray() y = email[&#39;spam&#39;] . len(X) . 5728 . X.shape . (5728, 25607) . len(y) . 5728 . 0.6 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 2509) . 1. Training the Naive Bayes model on the Training set . 1.1.1 GaussianNB . from sklearn.naive_bayes import GaussianNB GN_classifier = GaussianNB() GN_classifier.fit(X_train, y_train) . GaussianNB() . 1.1.2 Score . GN_score = GN_classifier.score(X_test,y_test) GN_score . 0.9528795811518325 . y_GN_pred = GN_classifier.predict(X_test) #Predicting the Test set results . 1.2.1 MultinomialNB . from sklearn.naive_bayes import MultinomialNB MN_classifier = MultinomialNB() MN_classifier.fit(X_train, y_train) . MultinomialNB() . 1.2.2 Score . MN_score = MN_classifier.score(X_test,y_test) MN_score . 0.9842931937172775 . y_MN_pred = MN_classifier.predict(X_test) #Predicting the Test set results . 2. Making the Confusion Matrix . 2.1 GaussianNB . from sklearn.metrics import confusion_matrix GN_cm = confusion_matrix(y_test, y_GN_pred) print(GN_cm) . [[849 26] [ 28 243]] . 2.2 MultinomialNB . from sklearn.metrics import confusion_matrix MN_cm = confusion_matrix(y_test, y_MN_pred) print(MN_cm) . [[862 13] [ 5 266]] . 3. Compare Both models . models = pd.DataFrame({&quot;GaussianNB&quot;: GN_score, &quot;MultinomialNB&quot;: MN_score }, index=[0]) models.T.plot.bar(title=&quot;Comapre different models&quot;, legend=False) plt.xticks(rotation=0); . 4. SAVE MODEL . import pickle pickle.dump(MN_classifier,open(&quot;Email_spam_naive_bayes_MN.pkl&quot;,&quot;wb&quot;)) .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/classification/2020/10/25/Naive-Bayes.html",
            "relUrl": "/jupyter/classification/2020/10/25/Naive-Bayes.html",
            "date": " • Oct 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Kernel SVM",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Social_Network_Ads.csv&#39;) dataset . User ID Gender Age EstimatedSalary Purchased . 0 15624510 | Male | 19 | 19000 | 0 | . 1 15810944 | Male | 35 | 20000 | 0 | . 2 15668575 | Female | 26 | 43000 | 0 | . 3 15603246 | Female | 27 | 57000 | 0 | . 4 15804002 | Male | 19 | 76000 | 0 | . ... ... | ... | ... | ... | ... | . 395 15691863 | Female | 46 | 41000 | 1 | . 396 15706071 | Male | 51 | 23000 | 1 | . 397 15654296 | Female | 50 | 20000 | 1 | . 398 15755018 | Male | 36 | 33000 | 0 | . 399 15594041 | Female | 49 | 36000 | 1 | . 400 rows × 5 columns . 0.3 Check if any null value . dataset.isna().sum() . User ID 0 Gender 0 Age 0 EstimatedSalary 0 Purchased 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 400 entries, 0 to 399 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 User ID 400 non-null int64 1 Gender 400 non-null object 2 Age 400 non-null int64 3 EstimatedSalary 400 non-null int64 4 Purchased 400 non-null int64 dtypes: int64(4), object(1) memory usage: 15.8+ KB . Drop User ID . dataset.drop(&#39;User ID&#39;, axis=1, inplace=True) dataset.head() . Gender Age EstimatedSalary Purchased . 0 Male | 19 | 19000 | 0 | . 1 Male | 35 | 20000 | 0 | . 2 Female | 26 | 43000 | 0 | . 3 Female | 27 | 57000 | 0 | . 4 Male | 19 | 76000 | 0 | . 0.4 Split into X &amp; y . X = dataset.drop(&#39;Purchased&#39;, axis=1) X.head() . Gender Age EstimatedSalary . 0 Male | 19 | 19000 | . 1 Male | 35 | 20000 | . 2 Female | 26 | 43000 | . 3 Female | 27 | 57000 | . 4 Male | 19 | 76000 | . y = dataset[&#39;Purchased&#39;] y.head() . 0 0 1 0 2 0 3 0 4 0 Name: Purchased, dtype: int64 . 0.5 Convert categories into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_feature = [&quot;Gender&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_feature)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) . pd.DataFrame(transformed_X).head() . 0 1 2 3 . 0 0.0 | 1.0 | 19.0 | 19000.0 | . 1 0.0 | 1.0 | 35.0 | 20000.0 | . 2 1.0 | 0.0 | 26.0 | 43000.0 | . 3 1.0 | 0.0 | 27.0 | 57000.0 | . 4 0.0 | 1.0 | 19.0 | 76000.0 | . 0.6 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size = 0.25, random_state = 2509) . 0.7 Feature Scaling . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) . 1.Training the model on the Training set . from sklearn.svm import SVC classifier = SVC(kernel = &#39;rbf&#39;) classifier.fit(X_train, y_train) . SVC() . 1.1 Score . classifier.score(X_test,y_test) . 0.92 . 2.Predicting the Test set results . y_pred = classifier.predict(X_test) . 2.2 Making the Confusion Matrix . from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_test, y_pred) print(cm) . [[65 5] [ 3 27]] .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/classification/2020/10/24/kernel_svm.html",
            "relUrl": "/jupyter/classification/2020/10/24/kernel_svm.html",
            "date": " • Oct 24, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Support Vector Regression (SVR)",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Position_Salaries.csv&#39;) dataset . Position Level Salary . 0 Business Analyst | 1 | 45000 | . 1 Junior Consultant | 2 | 50000 | . 2 Senior Consultant | 3 | 60000 | . 3 Manager | 4 | 80000 | . 4 Country Manager | 5 | 110000 | . 5 Region Manager | 6 | 150000 | . 6 Partner | 7 | 200000 | . 7 Senior Partner | 8 | 300000 | . 8 C-level | 9 | 500000 | . 9 CEO | 10 | 1000000 | . 0.3 Split into X &amp; y . X = dataset.iloc[:, 1:-1].values y = dataset.iloc[:, -1].values . y = y.reshape(len(y),1) . 0.4 Feature Scaling . from sklearn.preprocessing import StandardScaler sc_X = StandardScaler() sc_y = StandardScaler() X = sc_X.fit_transform(X) y = sc_y.fit_transform(y) X . array([[-1.5666989 ], [-1.21854359], [-0.87038828], [-0.52223297], [-0.17407766], [ 0.17407766], [ 0.52223297], [ 0.87038828], [ 1.21854359], [ 1.5666989 ]]) . 1. Training the model on the whole dataset . from sklearn.svm import SVR regressor = SVR(kernel = &#39;rbf&#39;) regressor.fit(X, y) . C: VIVEK 1.PYTHON_DEV env tensor lib site-packages sklearn utils validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) . SVR() . 2. Predicting a new result with SVR . sc_y.inverse_transform(regressor.predict(sc_X.transform([[6.5]]))) . array([170370.0204065]) . 3. Visualising the SVR results . X_grid = np.arange(min(sc_X.inverse_transform(X)), max(sc_X.inverse_transform(X)), 0.1) X_grid = X_grid.reshape((len(X_grid), 1)) plt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color = &#39;red&#39;) plt.plot(X_grid, sc_y.inverse_transform(regressor.predict(sc_X.transform(X_grid))), color = &#39;blue&#39;) plt.title(&#39;SVR&#39;) plt.xlabel(&#39;Position level&#39;) plt.ylabel(&#39;Salary&#39;) plt.show() .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/regression/2020/10/24/Support-Vector-Regression-(SVR).html",
            "relUrl": "/jupyter/regression/2020/10/24/Support-Vector-Regression-(SVR).html",
            "date": " • Oct 24, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Polynomial Regression",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Position_Salaries.csv&#39;) dataset . Position Level Salary . 0 Business Analyst | 1 | 45000 | . 1 Junior Consultant | 2 | 50000 | . 2 Senior Consultant | 3 | 60000 | . 3 Manager | 4 | 80000 | . 4 Country Manager | 5 | 110000 | . 5 Region Manager | 6 | 150000 | . 6 Partner | 7 | 200000 | . 7 Senior Partner | 8 | 300000 | . 8 C-level | 9 | 500000 | . 9 CEO | 10 | 1000000 | . 0.3 Split into X &amp; y . X = dataset.iloc[:, 1:-1].values y = dataset.iloc[:, -1].values . Training the Linear Regression model on the whole dataset . from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X, y) . LinearRegression() . Training the Polynomial Regression model on the whole dataset . from sklearn.preprocessing import PolynomialFeatures poly_reg = PolynomialFeatures(degree = 4) X_poly = poly_reg.fit_transform(X) lin_reg_2 = LinearRegression() lin_reg_2.fit(X_poly, y) . LinearRegression() . Visualising the Linear Regression results . plt.scatter(X, y, color = &#39;red&#39;) plt.plot(X, lin_reg.predict(X), color = &#39;blue&#39;) plt.title(&#39;Truth or Bluff (Linear Regression)&#39;) plt.xlabel(&#39;Position Level&#39;) plt.ylabel(&#39;Salary&#39;) plt.show() . Visualising the Polynomial Regression results . plt.scatter(X, y, color = &#39;red&#39;) plt.plot(X, lin_reg_2.predict(poly_reg.fit_transform(X)), color = &#39;blue&#39;) plt.title(&#39;Truth or Bluff (Polynomial Regression)&#39;) plt.xlabel(&#39;Position level&#39;) plt.ylabel(&#39;Salary&#39;) plt.show() . Predicting a new result with Linear Regression . lin_reg.predict([[6.5]]) . array([330378.78787879]) . Predicting a new result with Polynomial Regression . lin_reg_2.predict(poly_reg.fit_transform([[6.5]])) . array([158862.45265153]) .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/regression/2020/10/23/polynomial_regression.html",
            "relUrl": "/jupyter/regression/2020/10/23/polynomial_regression.html",
            "date": " • Oct 23, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Supprt Vector Machine (SVM)",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Social_Network_Ads.csv&#39;) dataset . User ID Gender Age EstimatedSalary Purchased . 0 15624510 | Male | 19 | 19000 | 0 | . 1 15810944 | Male | 35 | 20000 | 0 | . 2 15668575 | Female | 26 | 43000 | 0 | . 3 15603246 | Female | 27 | 57000 | 0 | . 4 15804002 | Male | 19 | 76000 | 0 | . ... ... | ... | ... | ... | ... | . 395 15691863 | Female | 46 | 41000 | 1 | . 396 15706071 | Male | 51 | 23000 | 1 | . 397 15654296 | Female | 50 | 20000 | 1 | . 398 15755018 | Male | 36 | 33000 | 0 | . 399 15594041 | Female | 49 | 36000 | 1 | . 400 rows × 5 columns . 0.3 Check if any null value . dataset.isna().sum() . User ID 0 Gender 0 Age 0 EstimatedSalary 0 Purchased 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 400 entries, 0 to 399 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 User ID 400 non-null int64 1 Gender 400 non-null object 2 Age 400 non-null int64 3 EstimatedSalary 400 non-null int64 4 Purchased 400 non-null int64 dtypes: int64(4), object(1) memory usage: 15.8+ KB . Drop User ID . dataset.drop(&#39;User ID&#39;, axis=1, inplace=True) dataset.head() . Gender Age EstimatedSalary Purchased . 0 Male | 19 | 19000 | 0 | . 1 Male | 35 | 20000 | 0 | . 2 Female | 26 | 43000 | 0 | . 3 Female | 27 | 57000 | 0 | . 4 Male | 19 | 76000 | 0 | . 0.4 Split into X &amp; y . X = dataset.drop(&#39;Purchased&#39;, axis=1) X.head() . Gender Age EstimatedSalary . 0 Male | 19 | 19000 | . 1 Male | 35 | 20000 | . 2 Female | 26 | 43000 | . 3 Female | 27 | 57000 | . 4 Male | 19 | 76000 | . y = dataset[&#39;Purchased&#39;] y.head() . 0 0 1 0 2 0 3 0 4 0 Name: Purchased, dtype: int64 . 0.5 Convert categories into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_feature = [&quot;Gender&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_feature)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) . pd.DataFrame(transformed_X).head() . 0 1 2 3 . 0 0.0 | 1.0 | 19.0 | 19000.0 | . 1 0.0 | 1.0 | 35.0 | 20000.0 | . 2 1.0 | 0.0 | 26.0 | 43000.0 | . 3 1.0 | 0.0 | 27.0 | 57000.0 | . 4 0.0 | 1.0 | 19.0 | 76000.0 | . 0.6 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size = 0.25, random_state = 2509) . 0.7 Feature Scaling . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) . 1.Training the model on the Training set . from sklearn.svm import SVC classifier = SVC(kernel = &#39;linear&#39;, random_state = 0) classifier.fit(X_train, y_train) . SVC(kernel=&#39;linear&#39;, random_state=0) . 1.1 Score . classifier.score(X_test,y_test) . 0.88 . 2.Predicting the Test set results . y_pred = classifier.predict(X_test) . 2.2 Making the Confusion Matrix . from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_test, y_pred) print(cm) . [[65 5] [ 7 23]] .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/classification/2020/10/23/Supprt_Vector_Machine.html",
            "relUrl": "/jupyter/classification/2020/10/23/Supprt_Vector_Machine.html",
            "date": " • Oct 23, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Multiple Linear Regression",
            "content": "0.Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;50_Startups.csv&#39;) dataset . R&amp;D Spend Administration Marketing Spend State Profit . 0 165349.20 | 136897.80 | 471784.10 | New York | 192261.83 | . 1 162597.70 | 151377.59 | 443898.53 | California | 191792.06 | . 2 153441.51 | 101145.55 | 407934.54 | Florida | 191050.39 | . 3 144372.41 | 118671.85 | 383199.62 | New York | 182901.99 | . 4 142107.34 | 91391.77 | 366168.42 | Florida | 166187.94 | . 5 131876.90 | 99814.71 | 362861.36 | New York | 156991.12 | . 6 134615.46 | 147198.87 | 127716.82 | California | 156122.51 | . 7 130298.13 | 145530.06 | 323876.68 | Florida | 155752.60 | . 8 120542.52 | 148718.95 | 311613.29 | New York | 152211.77 | . 9 123334.88 | 108679.17 | 304981.62 | California | 149759.96 | . 10 101913.08 | 110594.11 | 229160.95 | Florida | 146121.95 | . 11 100671.96 | 91790.61 | 249744.55 | California | 144259.40 | . 12 93863.75 | 127320.38 | 249839.44 | Florida | 141585.52 | . 13 91992.39 | 135495.07 | 252664.93 | California | 134307.35 | . 14 119943.24 | 156547.42 | 256512.92 | Florida | 132602.65 | . 15 114523.61 | 122616.84 | 261776.23 | New York | 129917.04 | . 16 78013.11 | 121597.55 | 264346.06 | California | 126992.93 | . 17 94657.16 | 145077.58 | 282574.31 | New York | 125370.37 | . 18 91749.16 | 114175.79 | 294919.57 | Florida | 124266.90 | . 19 86419.70 | 153514.11 | 0.00 | New York | 122776.86 | . 20 76253.86 | 113867.30 | 298664.47 | California | 118474.03 | . 21 78389.47 | 153773.43 | 299737.29 | New York | 111313.02 | . 22 73994.56 | 122782.75 | 303319.26 | Florida | 110352.25 | . 23 67532.53 | 105751.03 | 304768.73 | Florida | 108733.99 | . 24 77044.01 | 99281.34 | 140574.81 | New York | 108552.04 | . 25 64664.71 | 139553.16 | 137962.62 | California | 107404.34 | . 26 75328.87 | 144135.98 | 134050.07 | Florida | 105733.54 | . 27 72107.60 | 127864.55 | 353183.81 | New York | 105008.31 | . 28 66051.52 | 182645.56 | 118148.20 | Florida | 103282.38 | . 29 65605.48 | 153032.06 | 107138.38 | New York | 101004.64 | . 30 61994.48 | 115641.28 | 91131.24 | Florida | 99937.59 | . 31 61136.38 | 152701.92 | 88218.23 | New York | 97483.56 | . 32 63408.86 | 129219.61 | 46085.25 | California | 97427.84 | . 33 55493.95 | 103057.49 | 214634.81 | Florida | 96778.92 | . 34 46426.07 | 157693.92 | 210797.67 | California | 96712.80 | . 35 46014.02 | 85047.44 | 205517.64 | New York | 96479.51 | . 36 28663.76 | 127056.21 | 201126.82 | Florida | 90708.19 | . 37 44069.95 | 51283.14 | 197029.42 | California | 89949.14 | . 38 20229.59 | 65947.93 | 185265.10 | New York | 81229.06 | . 39 38558.51 | 82982.09 | 174999.30 | California | 81005.76 | . 40 28754.33 | 118546.05 | 172795.67 | California | 78239.91 | . 41 27892.92 | 84710.77 | 164470.71 | Florida | 77798.83 | . 42 23640.93 | 96189.63 | 148001.11 | California | 71498.49 | . 43 15505.73 | 127382.30 | 35534.17 | New York | 69758.98 | . 44 22177.74 | 154806.14 | 28334.72 | California | 65200.33 | . 45 1000.23 | 124153.04 | 1903.93 | New York | 64926.08 | . 46 1315.46 | 115816.21 | 297114.46 | Florida | 49490.75 | . 47 0.00 | 135426.92 | 0.00 | California | 42559.73 | . 48 542.05 | 51743.15 | 0.00 | New York | 35673.41 | . 49 0.00 | 116983.80 | 45173.06 | California | 14681.40 | . 0.3 Check if any null value . dataset.isna().sum() . R&amp;D Spend 0 Administration 0 Marketing Spend 0 State 0 Profit 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 50 entries, 0 to 49 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 R&amp;D Spend 50 non-null float64 1 Administration 50 non-null float64 2 Marketing Spend 50 non-null float64 3 State 50 non-null object 4 Profit 50 non-null float64 dtypes: float64(4), object(1) memory usage: 2.1+ KB . 0.4 Split into X &amp; y . X = dataset.drop(&#39;Profit&#39;, axis=1) X . R&amp;D Spend Administration Marketing Spend State . 0 165349.20 | 136897.80 | 471784.10 | New York | . 1 162597.70 | 151377.59 | 443898.53 | California | . 2 153441.51 | 101145.55 | 407934.54 | Florida | . 3 144372.41 | 118671.85 | 383199.62 | New York | . 4 142107.34 | 91391.77 | 366168.42 | Florida | . 5 131876.90 | 99814.71 | 362861.36 | New York | . 6 134615.46 | 147198.87 | 127716.82 | California | . 7 130298.13 | 145530.06 | 323876.68 | Florida | . 8 120542.52 | 148718.95 | 311613.29 | New York | . 9 123334.88 | 108679.17 | 304981.62 | California | . 10 101913.08 | 110594.11 | 229160.95 | Florida | . 11 100671.96 | 91790.61 | 249744.55 | California | . 12 93863.75 | 127320.38 | 249839.44 | Florida | . 13 91992.39 | 135495.07 | 252664.93 | California | . 14 119943.24 | 156547.42 | 256512.92 | Florida | . 15 114523.61 | 122616.84 | 261776.23 | New York | . 16 78013.11 | 121597.55 | 264346.06 | California | . 17 94657.16 | 145077.58 | 282574.31 | New York | . 18 91749.16 | 114175.79 | 294919.57 | Florida | . 19 86419.70 | 153514.11 | 0.00 | New York | . 20 76253.86 | 113867.30 | 298664.47 | California | . 21 78389.47 | 153773.43 | 299737.29 | New York | . 22 73994.56 | 122782.75 | 303319.26 | Florida | . 23 67532.53 | 105751.03 | 304768.73 | Florida | . 24 77044.01 | 99281.34 | 140574.81 | New York | . 25 64664.71 | 139553.16 | 137962.62 | California | . 26 75328.87 | 144135.98 | 134050.07 | Florida | . 27 72107.60 | 127864.55 | 353183.81 | New York | . 28 66051.52 | 182645.56 | 118148.20 | Florida | . 29 65605.48 | 153032.06 | 107138.38 | New York | . 30 61994.48 | 115641.28 | 91131.24 | Florida | . 31 61136.38 | 152701.92 | 88218.23 | New York | . 32 63408.86 | 129219.61 | 46085.25 | California | . 33 55493.95 | 103057.49 | 214634.81 | Florida | . 34 46426.07 | 157693.92 | 210797.67 | California | . 35 46014.02 | 85047.44 | 205517.64 | New York | . 36 28663.76 | 127056.21 | 201126.82 | Florida | . 37 44069.95 | 51283.14 | 197029.42 | California | . 38 20229.59 | 65947.93 | 185265.10 | New York | . 39 38558.51 | 82982.09 | 174999.30 | California | . 40 28754.33 | 118546.05 | 172795.67 | California | . 41 27892.92 | 84710.77 | 164470.71 | Florida | . 42 23640.93 | 96189.63 | 148001.11 | California | . 43 15505.73 | 127382.30 | 35534.17 | New York | . 44 22177.74 | 154806.14 | 28334.72 | California | . 45 1000.23 | 124153.04 | 1903.93 | New York | . 46 1315.46 | 115816.21 | 297114.46 | Florida | . 47 0.00 | 135426.92 | 0.00 | California | . 48 542.05 | 51743.15 | 0.00 | New York | . 49 0.00 | 116983.80 | 45173.06 | California | . y = dataset[&#39;Profit&#39;] y . 0 192261.83 1 191792.06 2 191050.39 3 182901.99 4 166187.94 5 156991.12 6 156122.51 7 155752.60 8 152211.77 9 149759.96 10 146121.95 11 144259.40 12 141585.52 13 134307.35 14 132602.65 15 129917.04 16 126992.93 17 125370.37 18 124266.90 19 122776.86 20 118474.03 21 111313.02 22 110352.25 23 108733.99 24 108552.04 25 107404.34 26 105733.54 27 105008.31 28 103282.38 29 101004.64 30 99937.59 31 97483.56 32 97427.84 33 96778.92 34 96712.80 35 96479.51 36 90708.19 37 89949.14 38 81229.06 39 81005.76 40 78239.91 41 77798.83 42 71498.49 43 69758.98 44 65200.33 45 64926.08 46 49490.75 47 42559.73 48 35673.41 49 14681.40 Name: Profit, dtype: float64 . 0.5 Encoding categorical data . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_feature = [&quot;State&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_feature)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) . pd.DataFrame(transformed_X).head() . 0 1 2 3 4 5 . 0 0.0 | 0.0 | 1.0 | 165349.20 | 136897.80 | 471784.10 | . 1 1.0 | 0.0 | 0.0 | 162597.70 | 151377.59 | 443898.53 | . 2 0.0 | 1.0 | 0.0 | 153441.51 | 101145.55 | 407934.54 | . 3 0.0 | 0.0 | 1.0 | 144372.41 | 118671.85 | 383199.62 | . 4 0.0 | 1.0 | 0.0 | 142107.34 | 91391.77 | 366168.42 | . 0.6 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size = 0.25, random_state = 2509) . 1. Training the Multiple Linear Regression model on the Training set . from sklearn.linear_model import LinearRegression regressor = LinearRegression() regressor.fit(X_train, y_train) . LinearRegression() . 1.1 Score . regressor.score(X_test,y_test) . 0.9840064291741994 . 2. Predicting the Test set results . y_pred = regressor.predict(X_test) . d = {&#39;y_pred&#39;: y_pred, &#39;y_test&#39;: y_test} . 2.1 Compare Predicted results . pd.DataFrame(d) . y_pred y_test . 32 98884.371543 | 97427.84 | . 33 100047.235184 | 96778.92 | . 47 47766.247901 | 42559.73 | . 9 154976.558305 | 149759.96 | . 37 91129.087779 | 89949.14 | . 8 151755.926389 | 152211.77 | . 23 112436.195860 | 108733.99 | . 24 113375.898676 | 108552.04 | . 17 130706.106786 | 125370.37 | . 1 189141.730655 | 191792.06 | . 39 85217.422839 | 81005.76 | . 22 116952.737156 | 110352.25 | . 46 60343.602070 | 49490.75 | .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/regression/2020/10/22/multiple_linear_regression.html",
            "relUrl": "/jupyter/regression/2020/10/22/multiple_linear_regression.html",
            "date": " • Oct 22, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "K-Nearest Neighbors (K-NN)",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Social_Network_Ads.csv&#39;) dataset . User ID Gender Age EstimatedSalary Purchased . 0 15624510 | Male | 19 | 19000 | 0 | . 1 15810944 | Male | 35 | 20000 | 0 | . 2 15668575 | Female | 26 | 43000 | 0 | . 3 15603246 | Female | 27 | 57000 | 0 | . 4 15804002 | Male | 19 | 76000 | 0 | . ... ... | ... | ... | ... | ... | . 395 15691863 | Female | 46 | 41000 | 1 | . 396 15706071 | Male | 51 | 23000 | 1 | . 397 15654296 | Female | 50 | 20000 | 1 | . 398 15755018 | Male | 36 | 33000 | 0 | . 399 15594041 | Female | 49 | 36000 | 1 | . 400 rows × 5 columns . 0.3 Check if any null value . dataset.isna().sum() . User ID 0 Gender 0 Age 0 EstimatedSalary 0 Purchased 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 400 entries, 0 to 399 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 User ID 400 non-null int64 1 Gender 400 non-null object 2 Age 400 non-null int64 3 EstimatedSalary 400 non-null int64 4 Purchased 400 non-null int64 dtypes: int64(4), object(1) memory usage: 15.8+ KB . Drop User ID . dataset.drop(&#39;User ID&#39;, axis=1, inplace=True) dataset.head() . Gender Age EstimatedSalary Purchased . 0 Male | 19 | 19000 | 0 | . 1 Male | 35 | 20000 | 0 | . 2 Female | 26 | 43000 | 0 | . 3 Female | 27 | 57000 | 0 | . 4 Male | 19 | 76000 | 0 | . 0.4 Split into X &amp; y . X = dataset.drop(&#39;Purchased&#39;, axis=1) X.head() . Gender Age EstimatedSalary . 0 Male | 19 | 19000 | . 1 Male | 35 | 20000 | . 2 Female | 26 | 43000 | . 3 Female | 27 | 57000 | . 4 Male | 19 | 76000 | . y = dataset[&#39;Purchased&#39;] y.head() . 0 0 1 0 2 0 3 0 4 0 Name: Purchased, dtype: int64 . 0.5 Convert categories into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_feature = [&quot;Gender&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_feature)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) . pd.DataFrame(transformed_X).head() . 0 1 2 3 . 0 0.0 | 1.0 | 19.0 | 19000.0 | . 1 0.0 | 1.0 | 35.0 | 20000.0 | . 2 1.0 | 0.0 | 26.0 | 43000.0 | . 3 1.0 | 0.0 | 27.0 | 57000.0 | . 4 0.0 | 1.0 | 19.0 | 76000.0 | . 0.6 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size = 0.25, random_state = 2509) . 0.7 Feature Scaling . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) . 1.Training the model on the Training set . from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier(n_neighbors = 5, metric = &#39;minkowski&#39;, p = 2) classifier.fit(X_train, y_train) . KNeighborsClassifier() . 1.1 Score . classifier.score(X_test,y_test) . 0.93 . 2.Predicting the Test set results . y_pred = classifier.predict(X_test) . 2.2 Making the Confusion Matrix . from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_test, y_pred) print(cm) . [[66 4] [ 3 27]] .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/classification/2020/10/22/k_nearest_neighbors.html",
            "relUrl": "/jupyter/classification/2020/10/22/k_nearest_neighbors.html",
            "date": " • Oct 22, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Simple Linear Regression",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Salary_Data.csv&#39;) dataset . YearsExperience Salary . 0 1.1 | 39343.0 | . 1 1.3 | 46205.0 | . 2 1.5 | 37731.0 | . 3 2.0 | 43525.0 | . 4 2.2 | 39891.0 | . 5 2.9 | 56642.0 | . 6 3.0 | 60150.0 | . 7 3.2 | 54445.0 | . 8 3.2 | 64445.0 | . 9 3.7 | 57189.0 | . 10 3.9 | 63218.0 | . 11 4.0 | 55794.0 | . 12 4.0 | 56957.0 | . 13 4.1 | 57081.0 | . 14 4.5 | 61111.0 | . 15 4.9 | 67938.0 | . 16 5.1 | 66029.0 | . 17 5.3 | 83088.0 | . 18 5.9 | 81363.0 | . 19 6.0 | 93940.0 | . 20 6.8 | 91738.0 | . 21 7.1 | 98273.0 | . 22 7.9 | 101302.0 | . 23 8.2 | 113812.0 | . 24 8.7 | 109431.0 | . 25 9.0 | 105582.0 | . 26 9.5 | 116969.0 | . 27 9.6 | 112635.0 | . 28 10.3 | 122391.0 | . 29 10.5 | 121872.0 | . 0.3 Check if any null value . dataset.isna().sum() . YearsExperience 0 Salary 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 30 entries, 0 to 29 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 YearsExperience 30 non-null float64 1 Salary 30 non-null float64 dtypes: float64(2) memory usage: 608.0 bytes . 0.4 Split into X &amp; y . X = dataset.drop(&#39;Salary&#39;, axis=1) X . YearsExperience . 0 1.1 | . 1 1.3 | . 2 1.5 | . 3 2.0 | . 4 2.2 | . 5 2.9 | . 6 3.0 | . 7 3.2 | . 8 3.2 | . 9 3.7 | . 10 3.9 | . 11 4.0 | . 12 4.0 | . 13 4.1 | . 14 4.5 | . 15 4.9 | . 16 5.1 | . 17 5.3 | . 18 5.9 | . 19 6.0 | . 20 6.8 | . 21 7.1 | . 22 7.9 | . 23 8.2 | . 24 8.7 | . 25 9.0 | . 26 9.5 | . 27 9.6 | . 28 10.3 | . 29 10.5 | . y = dataset[&#39;Salary&#39;] y . 0 39343.0 1 46205.0 2 37731.0 3 43525.0 4 39891.0 5 56642.0 6 60150.0 7 54445.0 8 64445.0 9 57189.0 10 63218.0 11 55794.0 12 56957.0 13 57081.0 14 61111.0 15 67938.0 16 66029.0 17 83088.0 18 81363.0 19 93940.0 20 91738.0 21 98273.0 22 101302.0 23 113812.0 24 109431.0 25 105582.0 26 116969.0 27 112635.0 28 122391.0 29 121872.0 Name: Salary, dtype: float64 . 0.5 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0) . 1. Training the Simple Linear Regression model on the Training set . from sklearn.linear_model import LinearRegression regressor = LinearRegression() regressor.fit(X_train, y_train) . LinearRegression() . 1.1 Score . regressor.score(X_test,y_test) . 0.9740993407213511 . 2. Predicting the Test set results . y_pred = regressor.predict(X_test) . d = {&#39;y_pred&#39;: y_pred, &#39;y_test&#39;: y_test} . 2.1 Compare predicted results . pd.DataFrame(d) . y_pred y_test . 2 40817.783270 | 37731.0 | . 28 123188.082589 | 122391.0 | . 13 65154.462615 | 57081.0 | . 10 63282.410357 | 63218.0 | . 26 115699.873560 | 116969.0 | . 24 108211.664531 | 109431.0 | . 27 116635.899689 | 112635.0 | . 11 64218.436486 | 55794.0 | . 17 76386.776158 | 83088.0 | . 3. Visualising . 3.1 Visualising the Training set results . plt.scatter(X_train, y_train, color = &#39;red&#39;) plt.plot(X_train, regressor.predict(X_train), color = &#39;blue&#39;) plt.title(&#39;Salary vs Experience (Training set)&#39;) plt.xlabel(&#39;Years of Experience&#39;) plt.ylabel(&#39;Salary&#39;) plt.show() . 3.1 Visualising the Test set results . plt.scatter(X_test, y_test, color = &#39;red&#39;) plt.plot(X_train, regressor.predict(X_train), color = &#39;blue&#39;) plt.title(&#39;Salary vs Experience (Test set)&#39;) plt.xlabel(&#39;Years of Experience&#39;) plt.ylabel(&#39;Salary&#39;) plt.show() .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/regression/2020/10/21/simple_linear_regression.html",
            "relUrl": "/jupyter/regression/2020/10/21/simple_linear_regression.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Logistic Regression",
            "content": "0. Data Preprocessing . 0.1 Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . 0.2 Importing the dataset . dataset = pd.read_csv(&#39;Social_Network_Ads.csv&#39;) dataset . User ID Gender Age EstimatedSalary Purchased . 0 15624510 | Male | 19 | 19000 | 0 | . 1 15810944 | Male | 35 | 20000 | 0 | . 2 15668575 | Female | 26 | 43000 | 0 | . 3 15603246 | Female | 27 | 57000 | 0 | . 4 15804002 | Male | 19 | 76000 | 0 | . ... ... | ... | ... | ... | ... | . 395 15691863 | Female | 46 | 41000 | 1 | . 396 15706071 | Male | 51 | 23000 | 1 | . 397 15654296 | Female | 50 | 20000 | 1 | . 398 15755018 | Male | 36 | 33000 | 0 | . 399 15594041 | Female | 49 | 36000 | 1 | . 400 rows × 5 columns . 0.3 Check if any null value . dataset.isna().sum() . User ID 0 Gender 0 Age 0 EstimatedSalary 0 Purchased 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 400 entries, 0 to 399 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 User ID 400 non-null int64 1 Gender 400 non-null object 2 Age 400 non-null int64 3 EstimatedSalary 400 non-null int64 4 Purchased 400 non-null int64 dtypes: int64(4), object(1) memory usage: 15.8+ KB . dataset.drop(&#39;User ID&#39;, axis=1, inplace=True) . dataset.head() . Gender Age EstimatedSalary Purchased . 0 Male | 19 | 19000 | 0 | . 1 Male | 35 | 20000 | 0 | . 2 Female | 26 | 43000 | 0 | . 3 Female | 27 | 57000 | 0 | . 4 Male | 19 | 76000 | 0 | . 0.4 Split into X &amp; y . X = dataset.drop(&#39;Purchased&#39;, axis=1) X.head() . Gender Age EstimatedSalary . 0 Male | 19 | 19000 | . 1 Male | 35 | 20000 | . 2 Female | 26 | 43000 | . 3 Female | 27 | 57000 | . 4 Male | 19 | 76000 | . y = dataset[&#39;Purchased&#39;] y.head() . 0 0 1 0 2 0 3 0 4 0 Name: Purchased, dtype: int64 . 0.5 Convert categories into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_feature = [&quot;Gender&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_feature)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) . pd.DataFrame(transformed_X).head() . 0 1 2 3 . 0 0.0 | 1.0 | 19.0 | 19000.0 | . 1 0.0 | 1.0 | 35.0 | 20000.0 | . 2 1.0 | 0.0 | 26.0 | 43000.0 | . 3 1.0 | 0.0 | 27.0 | 57000.0 | . 4 0.0 | 1.0 | 19.0 | 76000.0 | . 0.6 Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size = 0.25, random_state = 2509) . 0.7 Feature Scaling . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) . 1. Training the model on the Training set . from sklearn.linear_model import LogisticRegression classifier = LogisticRegression() classifier.fit(X_train, y_train) . LogisticRegression() . 1.1 Score . LR_score = classifier.score(X_test, y_test) LR_score . 0.89 . 2. Predicting the Test set results . y_pred = classifier.predict(X_test) . 2.1 Making the Confusion Matrix . from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_test, y_pred) print(cm) . [[65 5] [ 6 24]] .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/classification/2020/10/21/logistic_regression.html",
            "relUrl": "/jupyter/classification/2020/10/21/logistic_regression.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Data Preprocessing",
            "content": "1.Import library . import numpy as np import pandas as pd import matplotlib.pyplot as plt . 2.Get the data . car_sales = pd.read_csv(&quot;car-sales-data.csv&quot;) car_sales.head() . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . 3.Check for missing values . car_sales.isna().sum() . Make 49 Colour 50 Odometer (KM) 50 Doors 50 Price 50 dtype: int64 . 3.1 What if data is filled with missing values? . Drop the rows with no labels | Fill them with some value(also known as imputation). | 1.Drop the rows with no labels . car_sales.dropna(subset=[&quot;Price&quot;],inplace=True) car_sales.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . car_sales . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . ... ... | ... | ... | ... | ... | . 995 Toyota | Black | 35820.0 | 4.0 | 32042.0 | . 996 NaN | White | 155144.0 | 3.0 | 5716.0 | . 997 Nissan | Blue | 66604.0 | 4.0 | 31570.0 | . 998 Honda | White | 215883.0 | 4.0 | 4001.0 | . 999 Toyota | Blue | 248360.0 | 4.0 | 12732.0 | . 950 rows × 5 columns . 2.Fill them with some value . Option 1. Fill missing data with pandas | Option 2. Fill missing data with scikit learn | . 2.1 Option 1. With Pandas . car_sales.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . car_sales.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 950 entries, 0 to 999 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 Make 903 non-null object 1 Colour 904 non-null object 2 Odometer (KM) 902 non-null float64 3 Doors 903 non-null float64 4 Price 950 non-null float64 dtypes: float64(3), object(2) memory usage: 44.5+ KB . car_sales[&quot;Make&quot;].fillna(&quot;missing&quot;,inplace=True) car_sales[&quot;Colour&quot;].fillna(&quot;missing&quot;,inplace=True) car_sales[&quot;Odometer (KM)&quot;].fillna(car_sales[&quot;Odometer (KM)&quot;].median(),inplace=True) car_sales[&quot;Doors&quot;].fillna(4,inplace=True) . car_sales . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . ... ... | ... | ... | ... | ... | . 995 Toyota | Black | 35820.0 | 4.0 | 32042.0 | . 996 missing | White | 155144.0 | 3.0 | 5716.0 | . 997 Nissan | Blue | 66604.0 | 4.0 | 31570.0 | . 998 Honda | White | 215883.0 | 4.0 | 4001.0 | . 999 Toyota | Blue | 248360.0 | 4.0 | 12732.0 | . 950 rows × 5 columns . car_sales.isna().sum() . Make 0 Colour 0 Odometer (KM) 0 Doors 0 Price 0 dtype: int64 . 2.2 Option 2. With scikit learn . car_sales_missing = pd.read_csv(&quot;car-sales-data.csv&quot;) car_sales_missing . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . ... ... | ... | ... | ... | ... | . 995 Toyota | Black | 35820.0 | 4.0 | 32042.0 | . 996 NaN | White | 155144.0 | 3.0 | 5716.0 | . 997 Nissan | Blue | 66604.0 | 4.0 | 31570.0 | . 998 Honda | White | 215883.0 | 4.0 | 4001.0 | . 999 Toyota | Blue | 248360.0 | 4.0 | 12732.0 | . 1000 rows × 5 columns . car_sales_missing.dropna(subset=[&quot;Price&quot;],inplace=True) car_sales_missing.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . car_sales_missing.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 950 entries, 0 to 999 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 Make 903 non-null object 1 Colour 904 non-null object 2 Odometer (KM) 902 non-null float64 3 Doors 903 non-null float64 4 Price 950 non-null float64 dtypes: float64(3), object(2) memory usage: 44.5+ KB . If you are use scikit learn to fill missing value, then you have to Split data into X and y . X = car_sales_missing.drop(&quot;Price&quot;,axis=1) y = car_sales_missing[&quot;Price&quot;] . from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer # Fill categorical values with &#39;missing&#39; &amp; numerical values with median cat_imputer = SimpleImputer(strategy=&quot;constant&quot;, fill_value=&quot;missing&quot;) door_imputer = SimpleImputer(strategy=&quot;constant&quot;, fill_value=4) num_imputer = SimpleImputer(strategy=&quot;median&quot;) # Define columns cat_features = [&quot;Make&quot;, &quot;Colour&quot;] door_feature = [&quot;Doors&quot;] num_features = [&quot;Odometer (KM)&quot;] # Create an imputer (something that fills missing data) imputer = ColumnTransformer([ (&quot;cat_imputer&quot;, cat_imputer, cat_features), (&quot;door_imputer&quot;, door_imputer, door_feature), (&quot;num_imputer&quot;, num_imputer, num_features) ]) filled_X = imputer.fit_transform(X) filled_X . array([[&#39;Honda&#39;, &#39;White&#39;, 4.0, 35431.0], [&#39;BMW&#39;, &#39;Blue&#39;, 5.0, 192714.0], [&#39;Honda&#39;, &#39;White&#39;, 4.0, 84714.0], ..., [&#39;Nissan&#39;, &#39;Blue&#39;, 4.0, 66604.0], [&#39;Honda&#39;, &#39;White&#39;, 4.0, 215883.0], [&#39;Toyota&#39;, &#39;Blue&#39;, 4.0, 248360.0]], dtype=object) . car_sales_filled = pd.DataFrame(filled_X, columns=[&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;, &quot;Odometer (KM)&quot;]) car_sales_filled.isna().sum() . Make 0 Colour 0 Doors 0 Odometer (KM) 0 dtype: int64 . car_sales_filled . Make Colour Doors Odometer (KM) . 0 Honda | White | 4 | 35431 | . 1 BMW | Blue | 5 | 192714 | . 2 Honda | White | 4 | 84714 | . 3 Toyota | White | 4 | 154365 | . 4 Nissan | Blue | 3 | 181577 | . ... ... | ... | ... | ... | . 945 Toyota | Black | 4 | 35820 | . 946 missing | White | 3 | 155144 | . 947 Nissan | Blue | 4 | 66604 | . 948 Honda | White | 4 | 215883 | . 949 Toyota | Blue | 4 | 248360 | . 950 rows × 4 columns . Convert categorical data into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_features = [&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_features)], remainder=&quot;passthrough&quot;) # Fill train and test values separately transformed_X = transformer.fit_transform(car_sales_filled) # Check transformed and filled X_train transformed_X.toarray() . array([[0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 3.54310e+04], [1.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00, 1.00000e+00, 1.92714e+05], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 8.47140e+04], ..., [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 1.00000e+00, 0.00000e+00, 6.66040e+04], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 2.15883e+05], [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 2.48360e+05]]) . Split data into train and test . from sklearn.model_selection import train_test_split np.random.seed(2509) X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.2) . X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((760, 15), (190, 15), (760,), (190,)) . Now data is in write shape to fit into model . Happy coding and have a great time learning how to make machines smarter. .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/data%20preprocessing/2020/10/20/Data_preprocessing.html",
            "relUrl": "/jupyter/data%20preprocessing/2020/10/20/Data_preprocessing.html",
            "date": " • Oct 20, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Basic machine learning path",
            "content": "Basic path to learn Practical Machine Learning. . Prerequisite . High school math(vectors, matrices, calculus, probability, and stats) | Basic Python Help. | Must have Patience to learn new things. | . . Motivation . Watch AI For Everyone By Andrew Ng . The meaning behind common AI terminology, including neural networks, machine learning, deep learning, and data science. | What AI realistically can–and cannot–do. | How to spot opportunities to apply AI to problems in your own organization. | What it feels like to build machine learning and data science projects. | How to work with an AI team and build an AI strategy in your company. | How to navigate ethical and societal discussions surrounding AI. | . | YouTube Originals AGE OF AI . How AI is used in real life. . | . Started learning . Step-0 . Understand basic of machine learning Supervised Learning | Unsupervised Learning | Classification and Regression | . | Learn python and some useful library Pandas Pandas is a popular Python library for data analysis. . | NumPy NumPy is a very popular python library for large multi-dimensional array and matrix processing. . | Matplotlib Matpoltlib is a very popular Python library for data visualization. . | . | Setup Local Machine with latest Anaconda Anaconda is a free and open-source distribution of the Python and R. . | Step-1 . How to use data from verious source like [Kaggle UCI ml repo]. . | Start to use Jupyter notebook A complate IDE for data science and machine learning. . | Started hand on practice with scikit-learn. . | Use scikit-learn map and documentation. . | Step-2 . Now, you are a little bit comfortable with coding it’s time to learn basic maths behind those algorithms. . | Take a Andrew’s Course. . | . Step-3 . Learn about Deep-learning. . | Learn about popular library [TensorFlow PyTorch] TensorFlow is backed by Google Brain team. | PyTorch is developed by Facebook’s AI Research lab. | Both have large community. | There are other librarys as well like Theano, Keras, Caffe, Apache MXNet and many more. | . | In neural network learn ANN (artificial neural network) | CNN (Convolutional neural network) | RNN (Recurrent neural networks) | Autoencoder | | . . Happy coding and have a great time learning how to make machines smarter. .",
            "url": "https://vivek2509.github.io/World_of_ML/markdown/2020/10/19/Basic-ML-path.html",
            "relUrl": "/markdown/2020/10/19/Basic-ML-path.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Start writing blog",
            "content": "Start writing blog within 10 min . How to start write blog direct from Github using Markdown and Jupyter notebook. . . Requirement . github account. | . Follow step . Use fastai/fastpages template. Within 20-30 sec one pull request is generated by github-action. | Follow step and create SSH key from this and add public and private key. | Merge pull request and wait for 2-3 min for github-action to build the site. | Change Name and description . open _config.yml in edit mode. | change name and description | you can add social links as well | . . . . Adding stuff . Make sure are you add stuff into direct master OR main branch. | You can add notebook, markdown and words file into _notebook , _posts and _word folder accordingly with right name formet. | You can upload all your local images in images folder | . . . . Happy blogging .",
            "url": "https://vivek2509.github.io/World_of_ML/markdown/2020/10/18/How-to-start-blog-from-fastpage.html",
            "relUrl": "/markdown/2020/10/18/How-to-start-blog-from-fastpage.html",
            "date": " • Oct 18, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". AI/ML enthusiast with ♥ in photography 📸. .",
          "url": "https://vivek2509.github.io/World_of_ML/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://vivek2509.github.io/World_of_ML/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}